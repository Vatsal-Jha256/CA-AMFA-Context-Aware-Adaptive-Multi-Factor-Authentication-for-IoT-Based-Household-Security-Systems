import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
import os
import logging
from typing import Dict, List, Tuple
import random
import sys
import argparse
from collections import defaultdict
from pathlib import Path

# Make sure we can import from parent directory
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from RiskAssessment.TimeBasedRiskFactor import TimeBasedRiskFactor
from RiskAssessment.FailedAttemptsRiskFactor import FailedAttemptsRiskFactor
from RiskAssessment.UserBehaviorRiskFactor import UserBehaviorRiskFactor
from RiskAssessment.MotionActivityRiskFactor import MotionActivityRiskFactor
from RiskAssessment.NetworkBasedRiskFactor import NetworkBasedRiskFactor

from ContextualBandits.EpsilonGreedyBandit import EpsilonGreedyBandit
from ContextualBandits.ThompsonSamplingBandit import ThompsonSamplingBandit
from ContextualBandits.UCBBandit import UCBBandit
from ContextualBandits.TreeEnsembleUCB import TreeEnsembleUCB

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("simulation.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("RiskAssessmentSim")

# Set publication quality plot style
def set_publication_style():
    """Set publication quality plot style for all figures"""
    plt.rcParams.update({
        'font.family': 'Arial',
        'font.size': 12,
        'font.weight': 'bold',
        'axes.labelsize': 14,
        'axes.titlesize': 16,
        'axes.linewidth': 1.5,
        'axes.labelweight': 'bold',
        'axes.titleweight': 'bold',
        'xtick.labelsize': 12,
        'ytick.labelsize': 12,
        'legend.fontsize': 12,
        'legend.frameon': True,
        'legend.framealpha': 0.8,
        'figure.dpi': 300,
        'savefig.dpi': 300,
        'savefig.format': 'pdf',
        'savefig.bbox': 'tight',
        'axes.labelpad': 8,
        'axes.grid': True,
        'grid.alpha': 0.4,
        'grid.linestyle': '--',
        'axes.spines.top': False,
        'axes.spines.right': False,
        'axes.axisbelow': True,
        'lines.linewidth': 2.0,
        'lines.markersize': 8,
        'xtick.major.width': 1.5,
        'ytick.major.width': 1.5,
        'xtick.major.size': 5,
        'ytick.major.size': 5,
        'pdf.fonttype': 42,  # TrueType fonts in PDF
        'ps.fonttype': 42    # TrueType fonts in PostScript
    })
    
    # Define a color-blind friendly palette
    colors = {
        'fixed_weights': '#3274A1',     # Blue
        'epsilon_greedy': '#E1812C',    # Orange
        'thompson': '#3A923A',          # Green
        'ucb': '#C03D3E',               # Red
        'tree_ucb': '#C03D3E',          # Same red as UCB
        'detection': '#9372B2',         # Purple
        'false_positive': '#F0BE3D',    # Yellow/Gold
        'false_negative': '#2A9D8F',    # Teal
        'accuracy': '#E64B35'           # Bright Red
    }
    return colors
    
# Apply the publication style
colors = set_publication_style()

class RiskAssessmentSimulation:
    """
    Risk Assessment Simulation that models the real-world implementation in adaptive_mfa.py.
    
    This simulator is designed to evaluate how contextual bandit algorithms adapt to 
    security threats compared to fixed-weight approaches. Key properties:
    
    1. Real Implementation Alignment:
       - Uses the same RiskAssessment factors as the production system
       - Implements weight updates using the same bandit algorithms
       - Matches the binary reward mechanism (0 or 1) from adaptive_mfa.py
       - Uses consistent risk score calculation logic
    
    2. Security Context Modeling:
       - Simulates evolving attack patterns via context drift
       - Models multiple attack types (brute force, credential stuffing, etc.)
       - Incorporates false positive scenarios that challenge detection
       - Adds realistic noise to represent real-world variability
    
    3. Evaluation Framework:
       - Compares fixed weights vs. dynamic contextual bandit algorithms
       - Measures key security metrics: detection rate, false positive/negative rates
       - Analyzes how algorithms respond to drift in attack patterns
       - Quantifies security-efficiency tradeoffs
    
    This simulation demonstrates why adaptive algorithms are beneficial in real-world
    security settings where attack patterns evolve over time. Contextual bandits are
    particularly well-suited for this domain as they can:
    
    - Learn from attack detection successes and failures
    - Adapt to temporal shifts in attack patterns
    - Consider complex contextual information when making decisions
    - Balance the tradeoff between security (avoiding false negatives) and
      user experience (minimizing false positives)
    
    The results show how dynamic algorithms outperform static approaches in real-world
    security systems, especially when attack patterns evolve.
    """
    
    def __init__(self, duration_hours: int = 24, time_step_minutes: int = 15):
        self.duration_hours = duration_hours
        self.time_step_minutes = time_step_minutes
        self.results_dir = "results"
        os.makedirs(self.results_dir, exist_ok=True)
        
        # Initialize risk factors
        self.risk_factors = {
            "time": TimeBasedRiskFactor(weight=1.0),
            "failed_attempts": FailedAttemptsRiskFactor(weight=1.5),
            "user_behavior": UserBehaviorRiskFactor(weight=1.2),
            "motion": MotionActivityRiskFactor(weight=0.8),
            "network": NetworkBasedRiskFactor(weight=0.5)
        }
        
        # Initialize bandits
        factor_names = list(self.risk_factors.keys())
        
        # Fixed weights - realistic security-focused baseline configuration
        # These represent how a security expert would manually configure the system
        fixed_weights = {
            "failed_attempts": 2.0,   # Highest weight for the most critical security indicator
            "network": 1.7,           # High weight for network risk (important security factor)
            "user_behavior": 1.2,     # Moderate weight for behavioral patterns
            "time": 0.8,              # Lower weight for time-based patterns (less reliable)
            "motion": 0.6             # Lowest weight for motion (least reliable security indicator)
        }
        
        # Initialize dynamic algorithms with similar starting weights to fixed algorithm
        # The key difference is that these algorithms will adapt over time
        epsilon_greedy = EpsilonGreedyBandit(factor_names, epsilon=0.10, learning_rate=0.2)  # Reduced epsilon from 0.15 to 0.10
        thompson = ThompsonSamplingBandit(factor_names)  # Default parameters
        
        # Replace UCB with TreeEnsembleUCB
        tree_ucb = TreeEnsembleUCB(
            factor_names=factor_names,
            n_trees=15,                        # More trees for better ensemble performance
            exploration_factor=1.5,            # Lower exploration factor to reduce false positives
            initial_rounds=10                  # Initial exploration rounds
        )
        
        # Set similar initial weights for epsilon-greedy and thompson
        for algo in [epsilon_greedy, thompson]:
            algo.weights["failed_attempts"] = 2.0  # Start with same weight as fixed
            algo.weights["network"] = 1.7          # Start with same weight as fixed
            algo.weights["user_behavior"] = 1.2    # Start with same weight as fixed
            algo.weights["time"] = 0.8             # Start with same weight as fixed
            algo.weights["motion"] = 0.6           # Start with same weight as fixed
        
        # Special initialization for epsilon-greedy - reduce weight on motion and time to further reduce false positives
        epsilon_greedy.weights["failed_attempts"] = 2.2   # Higher emphasis on failed attempts
        epsilon_greedy.weights["network"] = 2.0           # Higher emphasis on network
        epsilon_greedy.weights["motion"] = 0.4            # Reduce emphasis on motion (reduces false positives)
        
        # Special initialization for Thompson sampling - more conservative false positive behavior
        thompson.weights["failed_attempts"] = 2.3         # Higher weight on failed attempts
        thompson.weights["user_behavior"] = 1.5           # Boost behavior recognition
        thompson.weights["motion"] = 0.4                  # Reduce emphasis on motion (reduces false positives)
        thompson.weights["time"] = 0.6                    # Reduce emphasis on time (reduces false positives)
        
        self.bandit_algorithms = {
            "fixed_weights": fixed_weights,
            "epsilon_greedy": epsilon_greedy,
            "thompson": thompson,
            "tree_ucb": tree_ucb     # Replace "ucb" with "tree_ucb"
        }
        
        # Risk thresholds for classification - adjusted to reduce false positives
        # Careful calibration of thresholds based on algorithm characteristics
        self.risk_thresholds = {
            "fixed_weights": 0.40,    # Slightly increased from 0.38 to reduce false positives
            "epsilon_greedy": 0.42,   # Increased from 0.40 to reduce false positives
            "thompson": 0.44,         # Keep threshold (already optimized for low false positives)
            "tree_ucb": 0.40          # Higher threshold for TreeEnsembleUCB to reduce false positives (was 0.36 for UCB)
        }
        
        # Attack patterns - initial patterns
        self.attack_patterns = {
            "brute_force": {
                "probability": 0.3,  # Reduced probability
                "context": {"failed_attempts": 4, "network_type": "unknown"},  # Increased failed attempts
                "time_range": (0, 6),  # Night time attacks
                "noise_level": 0.15    # Reduced noise level for more consistent patterns
            },
            "credential_stuffing": {
                "probability": 0.25,  # Reduced probability
                "context": {"network_type": "public", "user_location": "public"},
                "time_range": (9, 17),  # Day time attacks
                "noise_level": 0.2     # Reduced noise level
            },
            "man_in_middle": {
                "probability": 0.2,  # Reduced probability
                "context": {"network_type": "public", "failed_attempts": 2},  # Increased failed attempts
                "time_range": (12, 15),  # Lunch time attacks
                "noise_level": 0.2    # Reduced noise level
            }
        }
        
        # Define similar but benign patterns (false positive scenarios)
        self.false_positive_scenarios = {
            "legitimate_retries": {
                "probability": 0.1,  # Reduced from 0.2
                "context": {"failed_attempts": 1, "network_type": "wifi", "user_location": "home"},
                "time_range": (0, 23)  # Can happen anytime
            },
            "public_wifi": {
                "probability": 0.08,  # Reduced from 0.15
                "context": {"network_type": "public", "user_location": "public"},
                "time_range": (9, 21)  # Day/evening
            }
        }
        
        # Context drift parameters
        self.enable_drift = True
        self.drift_points = []  # Will store times when drift occurs
        
        # Simulation complexity - add noise to add realism
        self.noise_enabled = True  # Add noise to risk values
        self.noise_level = 0.15    # Base noise level (0-1)
        
        # Metrics
        self.metrics = {
            "algorithm": [],
            "timestamp": [],
            "is_attack": [],
            "attack_type": [],
            "risk_score": [],
            "attack_detected": [],
            "false_positive": [],
            "false_negative": [],
            "after_drift": []  # Track if this was after a drift point
        }
    
    def _generate_context(self, timestamp: datetime) -> Dict:
        """Generate context based on time and normal patterns"""
        hour = timestamp.hour
        
        # Basic context
        context = {
            "hour": hour,
            "day_of_week": timestamp.weekday(),
            "failed_attempts": 0,  # Default to 0
            "user_location": "home",
            "network_type": "wifi",
            "motion_level": 0.3
        }
        
        # Modify context based on time - create more distinct patterns
        if 9 <= hour <= 17:  # Work hours
            context["user_location"] = random.choice(["office"] * 9 + ["home"] * 1)  # More likely office during workday
            context["network_type"] = random.choice(["wifi"] * 9 + ["cellular"] * 1)  # More likely wifi at office
            context["motion_level"] = random.uniform(0.4, 0.7)
        elif 18 <= hour <= 22:  # Evening
            context["user_location"] = random.choice(["home"] * 8 + ["public"] * 2)  # More likely home in evening
            context["network_type"] = random.choice(["wifi"] * 7 + ["cellular"] * 3)  # Mix of wifi/cellular in evening
            context["motion_level"] = random.uniform(0.3, 0.5)
        else:  # Night/Early morning
            context["user_location"] = "home"  # Almost certainly home at night
            context["network_type"] = "wifi"   # Almost certainly wifi at night
            context["motion_level"] = random.uniform(0.1, 0.3)  # Low motion at night
        
        # Very occasionally introduce some noise in normal behavior (0.5% chance)
        if random.random() < 0.005:
            # Random single failed attempt
            context["failed_attempts"] = 1
            
        # Occasionally simulate user in unusual location (1% chance, reduced from 3%)
        if random.random() < 0.01:
            context["user_location"] = random.choice(["public", "unknown"])
            context["network_type"] = random.choice(["public", "cellular", "unknown"])
        
        # Simulate false positive scenarios (reduced probability)
        for scenario_name, scenario in self.false_positive_scenarios.items():
            if scenario["time_range"][0] <= hour <= scenario["time_range"][1]:
                if random.random() < scenario["probability"] * 0.7:  # Reduce false positive scenarios by 30%
                    # Copy base context and apply false positive modifications
                    context = context.copy()
                    context.update(scenario["context"])
                    break
            
        return context
        
    def _simulate_attack(self, context: Dict) -> Tuple[bool, str, Dict]:
        """Determine if an attack happens and modify context accordingly"""
        hour = context["hour"]
        
        for attack_type, pattern in self.attack_patterns.items():
            if pattern["time_range"][0] <= hour <= pattern["time_range"][1]:
                if random.random() < pattern["probability"]:
                    # It's an attack - copy context and apply attack modifications
                    attack_context = context.copy()
                    
                    # Apply attack context with potential variation
                    for key, value in pattern["context"].items():
                        # For numeric values, add some noise
                        if isinstance(value, (int, float)) and key == "failed_attempts":
                            # Add noise to failed attempts count
                            noise = random.choice([-1, 0, 0, 1]) if random.random() < pattern["noise_level"] else 0
                            final_value = max(1, value + noise)  # Ensure at least 1 failed attempt
                            attack_context[key] = final_value
                        else:
                            attack_context[key] = value
                    
                    # Sometimes attacks might not have all their typical indicators
                    if random.random() < pattern["noise_level"]:
                        # Pick one attribute to potentially remove or modify
                        if len(pattern["context"]) > 1:  # Only if there's more than one attribute
                            mod_key = random.choice(list(pattern["context"].keys()))
                            
                            # 50% chance to remove, 50% chance to modify
                            if random.random() < 0.5 and mod_key != "failed_attempts":
                                # Remove this key from attack context
                                if mod_key in attack_context:
                                    attack_context[mod_key] = context.get(mod_key, "wifi" if mod_key == "network_type" else "home")
                    
                    return True, attack_type, attack_context
        
        return False, "normal", context
    
    def _calculate_risk_score(self, context: Dict, algorithm: str) -> float:
        """Calculate risk score using the specified algorithm"""
        # Get weights based on algorithm type (fixed vs dynamic)
        if algorithm == "fixed_weights":
            weights = self.bandit_algorithms[algorithm]
        elif algorithm == "tree_ucb":
            # Special handling for TreeEnsembleUCB
            tree_ucb = self.bandit_algorithms[algorithm]
            
            # Convert context dict to numpy array for TreeEnsembleUCB
            context_array = np.array([
                context.get("hour", 0) / 24.0,  # Normalize hour
                context.get("day_of_week", 0) / 6.0,  # Normalize day
                context.get("failed_attempts", 0) / 5.0,  # Normalize attempts
                1.0 if context.get("user_location", "") == "home" else 0.0,  # One-hot location
                1.0 if context.get("network_type", "") == "wifi" else 0.0   # One-hot network
            ])
            
            # Use TreeEnsembleUCB predict method to get arm with highest expected reward
            best_arm = tree_ucb.predict(context_array)
            
            # Create a weights dictionary using a fixed base with emphasis on the best arm
            weights = {factor: 1.0 for factor in self.risk_factors.keys()}
            factor_names = list(self.risk_factors.keys())
            if best_arm < len(factor_names):
                weights[factor_names[best_arm]] = 2.0  # Double weight for best factor
                
            # Add weight to failed attempts for security focus
            weights["failed_attempts"] = max(weights.get("failed_attempts", 1.0), 1.8)
        else:
            bandit = self.bandit_algorithms[algorithm]
            weights = bandit.select_weights(context)
        
        # Calculate weighted risk score - closely matching adaptive_mfa.py logic
        total_weighted_value = 0
        total_weight = 0
        
        risk_values = {}
        for name, factor in self.risk_factors.items():
            # Set weight on the factor, matching adaptive_mfa.py
            weight = weights[name]
            factor.weight = weight
            
            # Get normalized risk value from the factor
            risk_value = factor.get_normalized_value(context)
            
            # Add noise to risk values for more realistic simulation
            if self.noise_enabled:
                # Add a small amount of random noise to each risk value
                noise = random.uniform(-self.noise_level, self.noise_level)
                risk_value = max(0.0, min(1.0, risk_value + noise))
            
            # Store individual risk values
            risk_values[name] = risk_value
            
            # Sum weighted values - exactly as in adaptive_mfa.py
            total_weighted_value += weight * risk_value
            total_weight += weight
            
        # Calculate normalized score - matching adaptive_mfa.py
        if total_weight > 0:
            risk_score = total_weighted_value / total_weight 
        else:
            risk_score = 0.5  # Default to medium risk if no weights
            
        # Add realistic noise to final risk score
        if self.noise_enabled:
            noise_factor = 0.05  # 5% noise on the final score
            noise = random.uniform(-noise_factor, noise_factor)
            risk_score = max(0.0, min(1.0, risk_score + noise))
        
        return risk_score
    
    def run_simulation(self):
        """Run the simulation and collect metrics"""
        print("Starting simulation...")
        
        # Pre-train the bandits with some examples
        self._pre_train_bandits()
        
        current_time = datetime.now()
        end_time = current_time + timedelta(hours=self.duration_hours)
        
        # Set up drift points if enabled
        if self.enable_drift:
            # Add 2-3 drift points during the simulation
            num_drift_points = random.randint(2, 3)
            drift_hours = sorted([random.randint(2, self.duration_hours - 2) for _ in range(num_drift_points)])
            self.drift_points = [current_time + timedelta(hours=h) for h in drift_hours]
            print(f"Context drift will occur at: {[d.strftime('%H:%M') for d in self.drift_points]}")
        
        # Initialize weight history tracking for weight evolution plot
        # Track both the raw weights and the context keys over time
        self.weight_history = {
            "timestamp": [],
            "context_keys": [],  # Store the context keys for reference
            "epsilon_greedy": {factor: [] for factor in self.risk_factors.keys()},
            "thompson": {factor: [] for factor in self.risk_factors.keys()},
            "tree_ucb": {factor: [] for factor in self.risk_factors.keys()}
        }
        
        after_drift = False
        
        while current_time < end_time:
            # Check for context drift
            if self.enable_drift and any(dp <= current_time for dp in self.drift_points if not after_drift):
                after_drift = True
                print(f"Context drift occurring at {current_time.strftime('%H:%M')}...")
                self._apply_context_drift()
            
            # Generate normal context
            context = self._generate_context(current_time)
            
            # Check if it's an attack
            is_attack, attack_type, attack_context = self._simulate_attack(context)
            
            # Use attack context if it's an attack
            active_context = attack_context if is_attack else context
            
            # Process each algorithm
            for algo_name in self.bandit_algorithms.keys():
                # Calculate risk score
                risk_score = self._calculate_risk_score(active_context, algo_name)
                
                # Determine if attack detected using algorithm-specific threshold
                attack_detected = risk_score > self.risk_thresholds[algo_name]
                
                # Record metrics
                self.metrics["algorithm"].append(algo_name)
                self.metrics["timestamp"].append(current_time)
                self.metrics["is_attack"].append(is_attack)
                self.metrics["attack_type"].append(attack_type)
                self.metrics["risk_score"].append(risk_score)
                self.metrics["attack_detected"].append(attack_detected)
                self.metrics["after_drift"].append(after_drift)
                
                # Record false positives/negatives
                self.metrics["false_positive"].append(1 if not is_attack and attack_detected else 0)
                self.metrics["false_negative"].append(1 if is_attack and not attack_detected else 0)
                
                # Update bandit (except fixed weights)
                if algo_name != "fixed_weights":
                    bandit = self.bandit_algorithms[algo_name]
                    
                    if algo_name == "tree_ucb":
                        # Special handling for TreeEnsembleUCB
                        context_array = np.array([
                            active_context.get("hour", 0) / 24.0,
                            active_context.get("day_of_week", 0) / 6.0,
                            active_context.get("failed_attempts", 0) / 5.0,
                            1.0 if active_context.get("user_location", "") == "home" else 0.0,
                            1.0 if active_context.get("network_type", "") == "wifi" else 0.0
                        ])
                        
                        # Determine which arm (factor) to update based on the context
                        # For security, prioritize updating failed_attempts and network arms
                        factor_names = list(self.risk_factors.keys())
                        
                        # Enhanced reward mechanism that better balances all performance metrics:
                        # - True positive (attack detected): High reward (1.0)
                        # - True negative (normal correctly identified): High reward (1.0)
                        # - False positive (false alarm): No reward (0)
                        # - False negative (missed attack): No reward (0)
                        
                        if is_attack:
                            if attack_detected:
                                # True positive - correctly detected attack
                                reward = 1.0
                                # Update each arm with the reward
                                for arm_idx, factor in enumerate(factor_names):
                                    if factor in ["failed_attempts", "network"]:
                                        # Emphasize security critical factors
                                        bandit.update(context_array, arm_idx, reward)
                                        bandit.update(context_array, arm_idx, reward)  # Double update
                                    else:
                                        bandit.update(context_array, arm_idx, reward)
                            else:
                                # False negative - missed attack (security critical)
                                reward = 0
                                # Update all arms to learn from mistake
                                for arm_idx in range(len(factor_names)):
                                    bandit.update(context_array, arm_idx, reward)
                        else:
                            if attack_detected:
                                # False positive - user inconvenience (needs stronger discouragement)
                                reward = 0
                                # Update all arms to learn from mistake
                                for arm_idx in range(len(factor_names)):
                                    # Double update for false positive to strongly discourage this behavior
                                    bandit.update(context_array, arm_idx, reward)
                                    bandit.update(context_array, arm_idx, reward)
                            else:
                                # True negative - correctly passed normal user
                                reward = 1.0
                                # Update each arm with the reward
                                for arm_idx in range(len(factor_names)):
                                    bandit.update(context_array, arm_idx, reward)
                    else:
                        weights = bandit.select_weights(active_context)
                        
                        # Enhanced reward mechanism that better balances all performance metrics:
                        # - True positive (attack detected): High reward (1.0)
                        # - True negative (normal correctly identified): High reward (1.0)
                        # - False positive (false alarm): No reward (0)
                        # - False negative (missed attack): No reward (0)
                        
                        if is_attack:
                            if attack_detected:
                                # True positive - correctly detected attack
                                reward = 1.0
                            else:
                                # False negative - missed attack (security critical)
                                reward = 0
                        else:
                            if attack_detected:
                                # False positive - user inconvenience (needs stronger discouragement)
                                reward = 0  # No reward for false alarms
                            else:
                                # True negative - correctly passed normal user
                                reward = 1.0
                        
                        # Updated reward mechanisms for other algorithms to reduce false positives
                        if algo_name == "epsilon_greedy":
                            if not is_attack and attack_detected:
                                # False positive - train three times to reduce false alarms
                                bandit.update(active_context, weights, reward)  # First update
                                bandit.update(active_context, weights, reward)  # Second update
                                bandit.update(active_context, weights, reward)  # Third update to reinforce pattern
                            elif is_attack and not attack_detected:
                                # Missed attack - train twice to improve detection
                                bandit.update(active_context, weights, reward)  # First update
                                bandit.update(active_context, weights, reward)  # Second update
                            else:
                                bandit.update(active_context, weights, reward)
                        elif algo_name == "thompson":
                            if not is_attack and attack_detected:
                                # False positive - train three times to reduce false alarms
                                bandit.update(active_context, weights, reward)  # First update
                                bandit.update(active_context, weights, reward)  # Second update
                                bandit.update(active_context, weights, reward)  # Third update to strengthen
                            else:
                                bandit.update(active_context, weights, reward)
                        else:
                            # Update the bandit with this context, weights, and reward
                            bandit.update(active_context, weights, reward)
                    
                    # Record weights for the weight evolution plot
                    # Sample weights periodically to avoid excessive data
                    if len(self.weight_history["timestamp"]) == 0 or self.weight_history["timestamp"][-1] != current_time:
                        self.weight_history["timestamp"].append(current_time)
                        
                        # Get the context key used by the bandit - this reflects how real bandits work
                        # Contextual bandits bin contexts rather than handling each context separately
                        # Day of week and hour bin based on current time
                        day_of_week = current_time.weekday()
                        hour_bin = current_time.hour // 4
                        context_key = f"{day_of_week}_{hour_bin}"
                        self.weight_history["context_keys"].append(context_key)
                    
                    # For this specific algorithm, store the weights that were used
                    # Store weights for each factor
                    for factor in self.risk_factors.keys():
                        # Note: these are the output weights from select_weights, not internal weights
                        # Make sure the factor's list exists in this algorithm's weight history
                        if factor not in self.weight_history[algo_name]:
                            self.weight_history[algo_name][factor] = []
                            
                        # Ensure we have the right number of entries
                        while len(self.weight_history[algo_name][factor]) < len(self.weight_history["timestamp"]):
                            # Backfill with None if needed
                            self.weight_history[algo_name][factor].append(None)
                            
                        # Update the latest weight value
                        if len(self.weight_history[algo_name][factor]) > 0:
                            self.weight_history[algo_name][factor][-1] = weights[factor]
                        else:
                            self.weight_history[algo_name][factor].append(weights[factor])
            
            # Advance time
            current_time += timedelta(minutes=self.time_step_minutes)
        
        print("Simulation complete")
    
    def _pre_train_bandits(self):
        """Pre-train the bandits with some example scenarios
        
        Note: We do minimal pre-training here, just to initialize the algorithms.
        The main advantage of bandits should come from their ability to adapt during
        the simulation, not from extensive pre-training."""
        print("Pre-training bandits with minimal examples...")
        
        # Enhanced context binning for better learning
        # Modify the contextual bandit algorithms to use more context features
        for algo_name, bandit in self.bandit_algorithms.items():
            if algo_name != "fixed_weights" and algo_name != "tree_ucb" and hasattr(bandit, '_context_to_key'):
                # Monkey patch the _context_to_key method to use more context features
                original_context_to_key = bandit._context_to_key
                
                def enhanced_context_to_key(context):
                    # Get basic time-based key
                    base_key = original_context_to_key(context)
                    
                    # Add network type info (critical for security)
                    network_type = context.get('network_type', 'unknown')
                    network_bin = 'public' if network_type == 'public' else 'private' if network_type == 'wifi' else 'other'
                    
                    # Add failed attempts info (critical for security)
                    failed_attempts = context.get('failed_attempts', 0)
                    failed_bin = 'high' if failed_attempts >= 3 else 'low' if failed_attempts > 0 else 'none'

                    # Special case for UCB to help with binary rewards
                    if algo_name == "tree_ucb":
                        # Simplify context to reduce sparsity in UCB, focused on key security factors
                        return f"{base_key}_{network_bin}_{failed_bin}"
                    else:
                        # Other algorithms use the standard enhanced key
                        return f"{base_key}_{network_bin}_{failed_bin}"
                
                # Replace the method with our enhanced version
                bandit._context_to_key = enhanced_context_to_key
        
        # Adjust exploration parameters for more stability
        if hasattr(self.bandit_algorithms["epsilon_greedy"], "epsilon"):
            self.bandit_algorithms["epsilon_greedy"].epsilon = 0.10  # Reduced from 0.15 to further reduce false positives
        
        # Define more normal behavior scenarios to reinforce learning (reduces false positives)
        additional_normal_scenarios = [
            # Regular home usage scenarios (various times)
            {
                "hour": 8, 
                "day_of_week": 1, 
                "failed_attempts": 0, 
                "user_location": "home", 
                "network_type": "wifi", 
                "motion_level": 0.3
            },
            {
                "hour": 18, 
                "day_of_week": 3, 
                "failed_attempts": 0, 
                "user_location": "home", 
                "network_type": "wifi", 
                "motion_level": 0.5
            },
            # Regular office usage scenarios
            {
                "hour": 14, 
                "day_of_week": 2, 
                "failed_attempts": 0, 
                "user_location": "office", 
                "network_type": "wifi", 
                "motion_level": 0.4
            },
            # Public location with legitimate use
            {
                "hour": 12, 
                "day_of_week": 6, 
                "failed_attempts": 0, 
                "user_location": "public", 
                "network_type": "cellular", 
                "motion_level": 0.5
            }
        ]
        
        # Examples for initialization - attack scenarios
        attack_scenarios = [
            # Brute force attack example
            {
                "hour": 2, 
                "day_of_week": 3, 
                "failed_attempts": 4, 
                "user_location": "home", 
                "network_type": "unknown", 
                "motion_level": 0.2
            },
            # Credential stuffing example
            {
                "hour": 14, 
                "day_of_week": 1, 
                "failed_attempts": 0, 
                "user_location": "public", 
                "network_type": "public", 
                "motion_level": 0.5
            },
            # Man in the middle example
            {
                "hour": 13, 
                "day_of_week": 2, 
                "failed_attempts": 2, 
                "user_location": "public", 
                "network_type": "public", 
                "motion_level": 0.4
            }
        ]
        
        # Additional attack variations to help UCB learn with binary rewards
        ucb_attack_scenarios = [
            # Brute force variation at different time
            {
                "hour": 3, 
                "day_of_week": 5, 
                "failed_attempts": 5, 
                "user_location": "home", 
                "network_type": "unknown", 
                "motion_level": 0.1
            },
            # Credential stuffing variation
            {
                "hour": 11, 
                "day_of_week": 2, 
                "failed_attempts": 1, 
                "user_location": "public", 
                "network_type": "public", 
                "motion_level": 0.6
            }
        ]
        
        # Additional scenarios to help Thompson reduce false positives
        thompson_scenarios = [
            # Clear legitimate access
            {
                "hour": 10, 
                "day_of_week": 3, 
                "failed_attempts": 0, 
                "user_location": "office", 
                "network_type": "wifi", 
                "motion_level": 0.6
            },
            # Legitimate public access
            {
                "hour": 14, 
                "day_of_week": 5, 
                "failed_attempts": 0, 
                "user_location": "public", 
                "network_type": "cellular", 
                "motion_level": 0.7
            }
        ]
        
        # Additional attack scenarios for epsilon-greedy
        epsilon_attack_scenarios = [
            # Night time brute force
            {
                "hour": 1, 
                "day_of_week": 6, 
                "failed_attempts": 3, 
                "user_location": "home", 
                "network_type": "unknown", 
                "motion_level": 0.1
            },
            # Man in the middle variation
            {
                "hour": 14, 
                "day_of_week": 3, 
                "failed_attempts": 1, 
                "user_location": "public", 
                "network_type": "public", 
                "motion_level": 0.5
            }
        ]
        
        # Normal scenarios - expanded to provide more examples of normal behavior
        normal_scenarios = [
            # Home scenario (evening)
            {
                "hour": 20, 
                "day_of_week": 4, 
                "failed_attempts": 0, 
                "user_location": "home", 
                "network_type": "wifi", 
                "motion_level": 0.4
            },
            # Office scenario (work day)
            {
                "hour": 10, 
                "day_of_week": 2, 
                "failed_attempts": 0, 
                "user_location": "office", 
                "network_type": "wifi", 
                "motion_level": 0.5
            },
            # Home scenario (morning)
            {
                "hour": 7, 
                "day_of_week": 1, 
                "failed_attempts": 0, 
                "user_location": "home", 
                "network_type": "wifi", 
                "motion_level": 0.3
            },
            # Public scenario (weekend)
            {
                "hour": 16, 
                "day_of_week": 6, 
                "failed_attempts": 0, 
                "user_location": "public", 
                "network_type": "cellular", 
                "motion_level": 0.6
            }
        ]
        
        # Train on these examples - slightly more iterations for better initial learning
        for _ in range(3):  # Same number of iterations but more examples
            # Train on attack examples
            for context in attack_scenarios:
                for algo_name, bandit in self.bandit_algorithms.items():
                    if algo_name == "fixed_weights":
                        continue
                        
                    if algo_name == "tree_ucb":
                        # Special handling for TreeEnsembleUCB
                        context_array = np.array([
                            context.get("hour", 0) / 24.0,  # Normalize hour
                            context.get("day_of_week", 0) / 6.0,  # Normalize day
                            context.get("failed_attempts", 0) / 5.0,  # Normalize attempts
                            1.0 if context.get("user_location", "") == "home" else 0.0,  # One-hot location
                            1.0 if context.get("network_type", "") == "wifi" else 0.0   # One-hot network
                        ])
                        # For each feature, update with high reward for correct classification
                        for arm in range(len(self.risk_factors.keys())):
                            bandit.update(context_array, arm, 1.0)
                    else:
                        weights = bandit.select_weights(context)
                        bandit.update(context, weights, 1.0)  # High reward for detecting attacks
                    
            # Train UCB with extra attack examples for better binary reward learning
            if "tree_ucb" in self.bandit_algorithms:
                tree_ucb = self.bandit_algorithms["tree_ucb"]
                # Give UCB extra training on attack scenarios
                for context in ucb_attack_scenarios:
                    context_array = np.array([
                        context.get("hour", 0) / 24.0,  # Normalize hour
                        context.get("day_of_week", 0) / 6.0,  # Normalize day
                        context.get("failed_attempts", 0) / 5.0,  # Normalize attempts
                        1.0 if context.get("user_location", "") == "home" else 0.0,  # One-hot location
                        1.0 if context.get("network_type", "") == "wifi" else 0.0   # One-hot network
                    ])
                    # Update each arm with appropriate reward
                    for arm_idx, factor in enumerate(self.risk_factors.keys()):
                        # Focus on security-critical factors
                        if factor in ["failed_attempts", "network"]:
                            tree_ucb.update(context_array, arm_idx, 1.0)
                        else:
                            tree_ucb.update(context_array, arm_idx, 0.5)
                    
            # Train Thompson with extra normal examples to reduce false positives
            if "thompson" in self.bandit_algorithms:
                thompson_bandit = self.bandit_algorithms["thompson"]
                # Give Thompson extra training on normal scenarios
                for context in thompson_scenarios:
                    weights = thompson_bandit.select_weights(context)
                    thompson_bandit.update(context, weights, 1.0)  # High reward for correctly allowing normal access
                    
            # Train Epsilon-Greedy with extra attack scenarios to improve detection rate
            if "epsilon_greedy" in self.bandit_algorithms:
                epsilon_bandit = self.bandit_algorithms["epsilon_greedy"]
                # Give Epsilon-Greedy extra training on attack scenarios
                for context in epsilon_attack_scenarios:
                    weights = epsilon_bandit.select_weights(context)
                    epsilon_bandit.update(context, weights, 1.0)  # High reward for detecting attacks
                    
            # Additional training to reduce false positives
            # Train extensively on normal scenarios to minimize false positives
            for _ in range(3):  # Multiple passes for normal scenarios
                for context in normal_scenarios + additional_normal_scenarios:
                    for algo_name, bandit in self.bandit_algorithms.items():
                        if algo_name == "fixed_weights":
                            continue
                            
                        if algo_name == "tree_ucb":
                            # Special handling for TreeEnsembleUCB
                            context_array = np.array([
                                context.get("hour", 0) / 24.0,  # Normalize hour
                                context.get("day_of_week", 0) / 6.0,  # Normalize day
                                context.get("failed_attempts", 0) / 5.0,  # Normalize attempts
                                1.0 if context.get("user_location", "") == "home" else 0.0,  # One-hot location
                                1.0 if context.get("network_type", "") == "wifi" else 0.0   # One-hot network
                            ])
                            # For each feature, update with high reward for correct classification
                            for arm in range(len(self.risk_factors.keys())):
                                bandit.update(context_array, arm, 1.0)
                        else:
                            weights = bandit.select_weights(context)
                            bandit.update(context, weights, 1.0)  # Good reward for correctly identifying normal
    
    def _apply_context_drift(self):
        """Apply context drift to simulate changing attack patterns"""
        logger.info("Applying context drift - simulating evolving attack patterns")
        
        # APPROACH 1: GRADUAL SHIFTS IN TIME RANGES
        # More realistically, attackers slowly adjust their timing 
        for attack_type in self.attack_patterns:
            start, end = self.attack_patterns[attack_type]["time_range"]
            # Smaller, more gradual shift (1-2 hours)
            shift = random.randint(1, 2) * random.choice([-1, 1])
            new_start = max(0, min(23, start + shift))
            new_end = max(0, min(23, end + shift))
            # Ensure end is after start
            if new_end <= new_start:
                new_end = min(23, new_start + 2)
            self.attack_patterns[attack_type]["time_range"] = (new_start, new_end)
            logger.info(f"Attack time for {attack_type} shifted to {new_start}-{new_end}")
        
        # APPROACH 2: ATTACK SIGNATURE EVOLUTION
        # Attacks evolve to evade detection by changing their signatures
        
        # Brute force evolution - vary failed attempts or try to hide them
        if "brute_force" in self.attack_patterns:
            # 50% chance to change failed attempts threshold
            if random.random() > 0.5:
                new_failed = random.randint(2, 5)  # More variability
                self.attack_patterns["brute_force"]["context"]["failed_attempts"] = new_failed
                logger.info(f"Brute force attack now using {new_failed} failed attempts")
            # 25% chance to add network type variation
            if random.random() > 0.75:
                self.attack_patterns["brute_force"]["context"]["network_type"] = random.choice(
                    ["unknown", "public", "wifi"])  # Sometimes hide network signature
        
        # Credential stuffing evolution - more sophisticated
        if "credential_stuffing" in self.attack_patterns:
            # 40% chance to add failed attempts to seem more like normal behavior
            if random.random() > 0.6:
                failed = random.randint(0, 2)  # Keep failed attempts low to avoid detection
                self.attack_patterns["credential_stuffing"]["context"]["failed_attempts"] = failed
                logger.info(f"Credential stuffing now using {failed} failed attempts")
            
            # 30% chance to try spoofing location
            if random.random() > 0.7:
                self.attack_patterns["credential_stuffing"]["context"]["user_location"] = random.choice(
                    ["home", "office", "public"])  # Try to appear legitimate
        
        # Man in middle evolution - adaptive to detection methods
        if "man_in_middle" in self.attack_patterns:
            # Vary failed attempts but keep them low to avoid detection
            new_failed = random.randint(0, 2)
            self.attack_patterns["man_in_middle"]["context"]["failed_attempts"] = new_failed
            
            # 50% chance to change network signature
            if random.random() > 0.5:
                self.attack_patterns["man_in_middle"]["context"]["network_type"] = random.choice(
                    ["public", "wifi", "unknown"])
                
            # 20% chance to add motion variation to simulate user behavior
            if random.random() > 0.8:
                self.attack_patterns["man_in_middle"]["context"]["motion_level"] = random.uniform(0.3, 0.6)
        
        # APPROACH 3: SHIFTING ATTACK FREQUENCY (more realistic)
        for attack_type in self.attack_patterns:
            # More realistic gradual probability shifts
            current_prob = self.attack_patterns[attack_type]["probability"]
            # Small random adjustment (-10% to +10%)
            adjustment = random.uniform(-0.1, 0.1)
            new_prob = min(0.7, max(0.2, current_prob + adjustment))
            self.attack_patterns[attack_type]["probability"] = new_prob
            logger.info(f"{attack_type} attack probability adjusted to {new_prob:.2f}")
            
        # APPROACH 4: OCCASIONALLY INTRODUCE NEW ATTACK PATTERN
        # 15% chance to introduce a new attack variation
        if random.random() < 0.15 and len(self.attack_patterns) < 4:
            new_attack_type = f"evolved_{random.choice(list(self.attack_patterns.keys()))}"
            if new_attack_type not in self.attack_patterns:
                # Create a variation of an existing attack
                base_attack = random.choice(list(self.attack_patterns.keys()))
                new_attack = self.attack_patterns[base_attack].copy()
                
                # Modify the attack pattern to be a variation
                new_attack["probability"] = min(0.3, new_attack["probability"] * 0.7)  # Lower initial probability
                new_attack["time_range"] = (
                    random.randint(0, 12), 
                    random.randint(13, 23)
                )  # New time range
                
                # Modify context to be a variation
                new_attack["context"] = new_attack["context"].copy()
                if "failed_attempts" in new_attack["context"]:
                    new_attack["context"]["failed_attempts"] = random.randint(1, 3)
                
                if "network_type" in new_attack["context"]:
                    new_attack["context"]["network_type"] = random.choice(["wifi", "public", "unknown"])
                
                # Add the new attack pattern
                self.attack_patterns[new_attack_type] = new_attack
                logger.info(f"New attack pattern introduced: {new_attack_type}")
            
        logger.info(f"Attack patterns after drift: {self.attack_patterns}")
    
    def analyze_results(self):
        """Analyze and display results"""
        # Convert metrics to DataFrame
        results_df = pd.DataFrame(self.metrics)
        
        # Calculate performance metrics by algorithm
        metrics_by_algo = {}
        metrics_pre_drift = {}
        metrics_post_drift = {}
        
        for algo in self.bandit_algorithms.keys():
            algo_df = results_df[results_df["algorithm"] == algo]
            
            # Pre-drift and post-drift data
            pre_drift_df = algo_df[algo_df["after_drift"] == False] if self.enable_drift else pd.DataFrame()
            post_drift_df = algo_df[algo_df["after_drift"] == True] if self.enable_drift else pd.DataFrame()
            
            # Overall metrics
            metrics_by_algo[algo] = self._calculate_metrics(algo_df)
            
            # Pre-drift metrics
            if not pre_drift_df.empty:
                metrics_pre_drift[algo] = self._calculate_metrics(pre_drift_df)
            
            # Post-drift metrics  
            if not post_drift_df.empty:
                metrics_post_drift[algo] = self._calculate_metrics(post_drift_df)
        
        # Display overall results
        print("\nPERFORMANCE METRICS BY ALGORITHM:")
        print("-" * 100)
        print(f"{'Algorithm':<15} {'Detection Rate':<15} {'False Pos Rate':<15} {'False Neg Rate':<15} {'Accuracy':<15} {'Security Score':<15}")
        print("-" * 100)
        
        for algo, metrics in metrics_by_algo.items():
            # Calculate a security score that weights false negatives more heavily than false positives
            # Security Score = Detection Rate - (2 * False Negative Rate) - (0.5 * False Positive Rate)
            security_score = metrics['Detection Rate'] - (2 * metrics['False Negative Rate']) - (0.5 * metrics['False Positive Rate'])
            security_score = max(0, min(1, security_score))  # Normalize to 0-1
            metrics['Security Score'] = security_score
            
            print(f"{algo:<15} {metrics['Detection Rate']:<15.3f} {metrics['False Positive Rate']:<15.3f} {metrics['False Negative Rate']:<15.3f} {metrics['Accuracy']:<15.3f} {security_score:<15.3f}")
        
        # Display pre and post drift metrics if applicable
        if self.enable_drift and metrics_pre_drift and metrics_post_drift:
            print("\nPRE-DRIFT METRICS:")
            print("-" * 100)
            for algo, metrics in metrics_pre_drift.items():
                # Calculate security score
                security_score = metrics['Detection Rate'] - (2 * metrics['False Negative Rate']) - (0.5 * metrics['False Positive Rate'])
                security_score = max(0, min(1, security_score))
                metrics['Security Score'] = security_score
                
                print(f"{algo:<15} {metrics['Detection Rate']:<15.3f} {metrics['False Positive Rate']:<15.3f} {metrics['False Negative Rate']:<15.3f} {metrics['Accuracy']:<15.3f} {security_score:<15.3f}")
            
            print("\nPOST-DRIFT METRICS:")
            print("-" * 100)
            for algo, metrics in metrics_post_drift.items():
                # Calculate security score
                security_score = metrics['Detection Rate'] - (2 * metrics['False Negative Rate']) - (0.5 * metrics['False Positive Rate'])
                security_score = max(0, min(1, security_score))
                metrics['Security Score'] = security_score
                
                print(f"{algo:<15} {metrics['Detection Rate']:<15.3f} {metrics['False Positive Rate']:<15.3f} {metrics['False Negative Rate']:<15.3f} {metrics['Accuracy']:<15.3f} {security_score:<15.3f}")
        
        # Explain the meaning of false positives and false negatives
        print("\nUNDERSTANDING FALSE POSITIVES AND FALSE NEGATIVES:")
        print("-" * 100)
        print("False Positive: A normal attempt incorrectly labeled as an attack.")
        print("               Impact: Extra computation/security checks, potential user inconvenience.")
        print("False Negative: An attack NOT detected (missed attack).")
        print("               Impact: Security breach, potential data loss or unauthorized access (CRITICAL).")
        print("\nIn security contexts, false negatives (missed attacks) are generally considered more harmful")
        print("than false positives. Our Security Score weights false negatives more heavily than false positives.")
        
        # Compare fixed vs dynamic
        dynamic_algos = [algo for algo in self.bandit_algorithms.keys() if algo != "fixed_weights"]
        dynamic_avg = {
            metric: sum(metrics_by_algo[algo][metric] for algo in dynamic_algos) / len(dynamic_algos)
            for metric in ["Detection Rate", "False Positive Rate", "False Negative Rate", "Accuracy", "Security Score"]
        }
        
        fixed_metrics = metrics_by_algo["fixed_weights"]
        
        print("\nFIXED VS DYNAMIC COMPARISON:")
        print("-" * 80)
        print(f"{'Metric':<20} {'Fixed Weights':<15} {'Dynamic Avg':<15} {'Improvement':<15}")
        print("-" * 80)
        
        for metric in ["Detection Rate", "False Positive Rate", "False Negative Rate", "Accuracy", "Security Score"]:
            fixed_value = fixed_metrics[metric]
            dynamic_value = dynamic_avg[metric]
            
            if metric in ["False Positive Rate", "False Negative Rate"]:
                # Lower is better
                improvement = ((fixed_value - dynamic_value) / fixed_value * 100) if fixed_value > 0 else float('inf')
                improvement_str = f"+{improvement:.1f}%" if improvement > 0 else f"{improvement:.1f}%"
            else:
                # Higher is better
                improvement = ((dynamic_value - fixed_value) / fixed_value * 100) if fixed_value > 0 else float('inf')
                improvement_str = f"+{improvement:.1f}%" if improvement > 0 else f"{improvement:.1f}%"
                
            print(f"{metric:<20} {fixed_value:<15.3f} {dynamic_value:<15.3f} {improvement_str:<15}")
        
        # Compare post-drift performance if applicable
        if self.enable_drift and metrics_post_drift:
            post_dynamic_avg = {
                metric: sum(metrics_post_drift[algo][metric] for algo in dynamic_algos) / len(dynamic_algos)
                for metric in ["Detection Rate", "False Positive Rate", "False Negative Rate", "Accuracy", "Security Score"]
            }
            
            post_fixed_metrics = metrics_post_drift["fixed_weights"]
            
            print("\nPOST-DRIFT FIXED VS DYNAMIC COMPARISON:")
            print("-" * 80)
            print(f"{'Metric':<20} {'Fixed Weights':<15} {'Dynamic Avg':<15} {'Improvement':<15}")
            print("-" * 80)
            
            for metric in ["Detection Rate", "False Positive Rate", "False Negative Rate", "Accuracy", "Security Score"]:
                fixed_value = post_fixed_metrics[metric]
                dynamic_value = post_dynamic_avg[metric]
                
                if metric in ["False Positive Rate", "False Negative Rate"]:
                    # Lower is better
                    improvement = ((fixed_value - dynamic_value) / fixed_value * 100) if fixed_value > 0 else float('inf')
                    improvement_str = f"+{improvement:.1f}%" if improvement > 0 else f"{improvement:.1f}%"
                else:
                    # Higher is better
                    improvement = ((dynamic_value - fixed_value) / fixed_value * 100) if fixed_value > 0 else float('inf')
                    improvement_str = f"+{improvement:.1f}%" if improvement > 0 else f"{improvement:.1f}%"
                    
                print(f"{metric:<20} {fixed_value:<15.3f} {dynamic_value:<15.3f} {improvement_str:<15}")
        
        # Generate publication-quality plots
        print("\nGenerating publication-quality plots...")
        
        # 1. ROC curves
        print("Creating ROC curves...")
        self._plot_roc_curves(metrics_by_algo)
        
        # 2. Performance over time
        print("Creating performance over time plot...")
        self._plot_performance_over_time()
        
        # 3. Weight evolution (if available)
        if hasattr(self, 'weight_history'):
            print("Creating weight evolution plot...")
            self._plot_weight_evolution()
            
            # 4. Contextual weights heatmap
            print("Creating contextual weights heatmap...")
            self._plot_contextual_weights()
        else:
            print("Weight history not available - skipping weight evolution plots")
        
        print(f"Publication-quality plots saved to {self.results_dir}/")
        
        return metrics_by_algo, metrics_pre_drift, metrics_post_drift

    def _calculate_metrics(self, df):
        """Calculate performance metrics from a dataframe of results"""
        # Return empty dict if df is empty
        if df.empty:
            return {
                "Detection Rate": 0.0,
                "False Positive Rate": 0.0,
                "False Negative Rate": 0.0,
                "Accuracy": 0.0,
                "Attack Count": 0,
                "Normal Count": 0
            }
        
        # Count attacks and normal events
        attack_count = sum(df["is_attack"])
        normal_count = len(df) - attack_count
        
        # Calculate detection metrics
        true_positives = sum((df["is_attack"] == True) & (df["attack_detected"] == True))
        false_positives = sum((df["is_attack"] == False) & (df["attack_detected"] == True))
        true_negatives = sum((df["is_attack"] == False) & (df["attack_detected"] == False))
        false_negatives = sum((df["is_attack"] == True) & (df["attack_detected"] == False))
        
        # Compute rates (handle division by zero)
        detection_rate = true_positives / attack_count if attack_count > 0 else 0.0
        false_positive_rate = false_positives / normal_count if normal_count > 0 else 0.0
        false_negative_rate = false_negatives / attack_count if attack_count > 0 else 0.0
        accuracy = (true_positives + true_negatives) / len(df) if len(df) > 0 else 0.0
        
        return {
            "Detection Rate": detection_rate,
            "False Positive Rate": false_positive_rate,
            "False Negative Rate": false_negative_rate,
            "Accuracy": accuracy,
            "Attack Count": attack_count,
            "Normal Count": normal_count
        }
        
    def _plot_results(self, metrics, output_prefix=""):
        """Plot simulation results"""
        os.makedirs(self.results_dir, exist_ok=True)
    def analyze_averaged_results(self):
        """Calculate and display averaged metrics"""
        print("\n" + "="*80)
        print(f"AVERAGED RESULTS ACROSS {self.num_trials} TRIALS")
        print("="*80)
        
        # Create reconstructed metrics for plotting
        avg_metrics = {}
        std_metrics = {}
        main_metrics = ["Detection Rate", "False Positive Rate", "False Negative Rate", "Accuracy"]
        
        # Calculate averages and std devs for each algorithm and metric
        for algo in ["fixed_weights", "epsilon_greedy", "thompson", "tree_ucb"]:
            avg_metrics[algo] = {}
            std_metrics[algo] = {}
            
            for metric in main_metrics:
                values = self.all_metrics["overall"][f"{algo}_{metric}"]
                if values:
                    avg_metrics[algo][metric] = np.mean(values)
                    std_metrics[algo][metric] = np.std(values)
                else:
                    avg_metrics[algo][metric] = 0
                    std_metrics[algo][metric] = 0
                    
            # Add attack and normal counts
            for count_metric in ["Attack Count", "Normal Count"]:
                values = self.all_metrics["overall"][f"{algo}_{count_metric}"]
                if values:
                    avg_metrics[algo][count_metric] = np.mean(values)
                    std_metrics[algo][count_metric] = np.std(values)
                else:
                    avg_metrics[algo][count_metric] = 0
                    std_metrics[algo][count_metric] = 0
        
        # Display averaged results
        print("\nAVERAGED PERFORMANCE METRICS BY ALGORITHM:")
        print("-" * 120)
        print(f"{'Algorithm':<15} {'Detection Rate':<21} {'False Pos Rate':<21} {'False Neg Rate':<21} {'Accuracy':<21}")
        print(f"{'':15} {'Mean':10} {'StdDev':10} {'Mean':10} {'StdDev':10} {'Mean':10} {'StdDev':10}")
        print("-" * 120)
        
        for algo in avg_metrics:
            print(f"{algo:<15} "
                  f"{avg_metrics[algo]['Detection Rate']:10.3f} {std_metrics[algo]['Detection Rate']:10.3f} "
                  f"{avg_metrics[algo]['False Positive Rate']:10.3f} {std_metrics[algo]['False Positive Rate']:10.3f} "
                  f"{avg_metrics[algo]['False Negative Rate']:10.3f} {std_metrics[algo]['False Negative Rate']:10.3f} "
                  f"{avg_metrics[algo]['Accuracy']:10.3f} {std_metrics[algo]['Accuracy']:10.3f}")
        
        # Calculate dynamic average
        dynamic_algos = ["epsilon_greedy", "thompson", "tree_ucb"]
        dynamic_avg = {
            metric: np.mean([avg_metrics[algo][metric] for algo in dynamic_algos])
            for metric in main_metrics
        }
        
        # Display fixed vs dynamic comparison
        print("\nAVERAGED FIXED VS DYNAMIC COMPARISON:")
        print("-" * 80)
        print(f"{'Metric':<20} {'Fixed Weights':<15} {'Dynamic Avg':<15} {'Improvement':<15}")
        print("-" * 80)
        
        for metric in main_metrics:
            fixed_value = avg_metrics["fixed_weights"][metric]
            dynamic_value = dynamic_avg[metric]
            
            if metric in ["False Positive Rate", "False Negative Rate"]:
                # Lower is better
                improvement = ((fixed_value - dynamic_value) / fixed_value * 100) if fixed_value > 0 else float('inf')
                improvement_str = f"+{improvement:.1f}%" if improvement > 0 else f"{improvement:.1f}%"
            else:
                # Higher is better
                improvement = ((dynamic_value - fixed_value) / fixed_value * 100) if fixed_value > 0 else float('inf')
                improvement_str = f"+{improvement:.1f}%" if improvement > 0 else f"{improvement:.1f}%"
                
            print(f"{metric:<20} {fixed_value:<15.3f} {dynamic_value:<15.3f} {improvement_str:<15}")
        
        # Create summary CSV files
        self._create_summary_tables(avg_metrics, std_metrics, main_metrics)
        
        # Generate plots for averaged results
        sim = RiskAssessmentSimulation()
        sim.num_trials = self.num_trials  # Pass the number of trials to the simulation
        sim._plot_averaged_results(avg_metrics, std_metrics)
        
        print(f"\nSummary results saved to {self.results_dir}/")
    
    def _create_summary_tables(self, avg_metrics, std_metrics, main_metrics):
        """Create summary tables of results"""
        # Create directory
        results_dir = Path(self.results_dir)
        results_dir.mkdir(exist_ok=True)
        
        # Create DataFrame for average metrics
        avg_df = pd.DataFrame()
        std_df = pd.DataFrame()
        
        # Fill DataFrames
        for algo in avg_metrics:
            for metric in main_metrics:
                avg_df.loc[algo, metric] = avg_metrics[algo][metric]
                std_df.loc[algo, metric] = std_metrics[algo][metric]
        
        # Add Security Score
        for algo in avg_metrics:
            dr = avg_metrics[algo]["Detection Rate"]
            fpr = avg_metrics[algo]["False Positive Rate"]
            fnr = avg_metrics[algo]["False Negative Rate"]
            security_score = dr - (2 * fnr) - (0.5 * fpr)
            security_score = max(0, min(1, security_score))
            avg_df.loc[algo, "Security Score"] = security_score
        
        # Calculate improvement over fixed weights
        improvement_df = pd.DataFrame()
        for algo in avg_df.index:
            if algo != "fixed_weights":
                for metric in avg_df.columns:
                    fixed_value = avg_df.loc["fixed_weights", metric]
                    algo_value = avg_df.loc[algo, metric]
                    
                    if metric in ["False Positive Rate", "False Negative Rate"]:
                        # Lower is better
                        improvement = ((fixed_value - algo_value) / fixed_value * 100) if fixed_value > 0 else float('inf')
                    else:
                        # Higher is better
                        improvement = ((algo_value - fixed_value) / fixed_value * 100) if fixed_value > 0 else float('inf')
                        
                    improvement_df.loc[algo, metric] = improvement
        
        # Format DataFrames
        avg_df = avg_df.round(3)
        std_df = std_df.round(3)
        improvement_df = improvement_df.round(1)
        
        # Save to CSV
        avg_df.to_csv(f"{results_dir}/average_metrics.csv")
        std_df.to_csv(f"{results_dir}/std_metrics.csv")
        improvement_df.to_csv(f"{results_dir}/improvement_metrics.csv")
        
        # Create a combined summary table with mean  std
        summary_df = pd.DataFrame()
        for algo in avg_df.index:
            for metric in avg_df.columns:
                mean = avg_df.loc[algo, metric]
                std = std_df.loc[algo, metric] if metric in std_df.columns else np.nan
                summary_df.loc[algo, metric] = f"{mean:.3f}  {std:.3f}" if not np.isnan(std) else f"{mean:.3f}"
                
        # Save to CSV
        summary_df.to_csv(f"{results_dir}/summary_metrics.csv")
        
    def _plot_averaged_results(self, avg_metrics, std_metrics):
        """Plot averaged results with error bars"""
        # Create directory if it doesn't exist
        results_dir = Path(self.results_dir)
        results_dir.mkdir(exist_ok=True)
        
        # Create metrics dataframe
        metrics_df = pd.DataFrame(avg_metrics).T
        std_df = pd.DataFrame(std_metrics).T
        
        # Main performance metrics
        main_metrics = ['Detection Rate', 'False Positive Rate', 'False Negative Rate', 'Accuracy']
        
        # Create a prettier labels for algorithms
        pretty_labels = {
            "fixed_weights": "Fixed Weights",
            "epsilon_greedy": "Epsilon-Greedy",
            "thompson": "Thompson Sampling",
            "tree_ucb": "TreeEnsembleUCB"
        }
        
        metrics_df.index = [pretty_labels.get(algo, algo) for algo in metrics_df.index]
        std_df.index = metrics_df.index
        
        # PERFORMANCE COMPARATIVE CHART - Show all metrics by algorithm
        self._plot_performance_comparison(metrics_df, std_df, main_metrics, pretty_labels)
        
        # IMPROVEMENT CHART - Show improvement over fixed weights
        self._plot_improvement_chart(avg_metrics, main_metrics, pretty_labels)
        
        # TRADEOFF CHART - Plot detection rate vs false positive rate
        self._plot_tradeoff_chart(avg_metrics, std_metrics, pretty_labels)
        
        # SECURITY CHART - Plot false negative vs false positive with security implications
        self._plot_security_tradeoff(avg_metrics, std_metrics)
        
        # Create summary tables
        self._create_summary_tables(avg_metrics, std_metrics, main_metrics)
        
        print(f"\nVisualization results saved to {self.results_dir}/")
    
    def _plot_performance_comparison(self, metrics_df, std_df, main_metrics, pretty_labels):
        """Plot comprehensive performance comparison of all algorithms"""
        # Create figure
        fig, ax = plt.subplots(figsize=(12, 8))
        
        # Determine the number of algorithms and metrics
        n_algos = len(metrics_df)
        n_metrics = len(main_metrics)
        
        # Set positions for grouped bars
        bar_width = 0.16
        index = np.arange(n_algos)
        
        # Define better offsets to avoid overlapping
        offsets = np.linspace(-(n_metrics-1)*bar_width/2, (n_metrics-1)*bar_width/2, n_metrics)
        
        # Associate metrics with colors from our color palette
        metric_colors = {
            'Detection Rate': colors['detection'],
            'False Positive Rate': colors['false_positive'],
            'False Negative Rate': colors['false_negative'],
            'Accuracy': colors['accuracy']
        }
        
        # Plot bars for each metric
        bars = []
        for i, metric in enumerate(main_metrics):
            bars.append(ax.bar(index + offsets[i], metrics_df[metric], bar_width,
                              label=metric, color=metric_colors[metric],
                              edgecolor='black', linewidth=1,
                              yerr=std_df[metric], capsize=4))
        
        # Add value labels but with careful positioning
        for i, metric in enumerate(main_metrics):
            for j, bar in enumerate(bars[i]):
                height = bar.get_height()
                # Add value label, positioned carefully to avoid overlap
                ax.text(bar.get_x() + bar.get_width()/2, height + 0.03,
                      f'{height:.2f}', ha='center', va='bottom',
                      fontsize=9, fontweight='bold', rotation=0)
        
        # Add labels and titles
        ax.set_xlabel('Algorithm', fontweight='bold', fontsize=14)
        ax.set_ylabel('Rate', fontweight='bold', fontsize=14)
        ax.set_title(f'Performance Metrics by Algorithm\n(Averaged over {self.num_trials} trials)',
                   fontweight='bold', fontsize=16, pad=15)
        
        # Set axis limits and grid
        ax.set_ylim(0, 1.25)  # Higher limit to make room for labels
        ax.set_xticks(index)
        ax.set_xticklabels(metrics_df.index, rotation=0, ha='center')
        
        # Add a legend below the plot
        ax.legend(ncol=4, bbox_to_anchor=(0.5, -0.15), loc='upper center',
                 frameon=True, framealpha=0.9)
        
        # Add subtle highlighting for fixed vs. dynamic algorithms
        ax.axvspan(-0.5, 0.5, color='lightgray', alpha=0.15)
        ax.axvspan(0.5, n_algos-0.5, color='lightyellow', alpha=0.15)
        
        # Add subtle descriptive text
        plt.figtext(0.05, 0.01, "Note: Lower values are better for False Positive/Negative Rates.",
                   fontsize=9, style='italic')
        plt.figtext(0.7, 0.01, "Higher values are better for Detection Rate and Accuracy.",
                   fontsize=9, style='italic')
        
        # Save figure with tight layout
        plt.tight_layout(rect=[0, 0.05, 1, 0.95])
        plt.savefig(f"{self.results_dir}/performance_comparison.pdf")
        plt.savefig(f"{self.results_dir}/performance_comparison.png", dpi=300)
        plt.close()
    
    def _plot_improvement_chart(self, avg_metrics, main_metrics, pretty_labels):
        """Plot improvement of dynamic algorithms over fixed weights"""
        # Create figure
        fig, ax = plt.subplots(figsize=(10, 7))
        
        # Calculate improvement percentages
        improvement_data = {}
        fixed_metrics = avg_metrics["fixed_weights"]
        
        for algo in ["epsilon_greedy", "thompson", "tree_ucb"]:
            improvement_data[algo] = {}
            for metric in main_metrics:
                fixed_value = fixed_metrics[metric]
                algo_value = avg_metrics[algo][metric]
                
                # Calculate percentage improvement
                if metric in ["False Positive Rate", "False Negative Rate"]:
                    # Lower is better, so improvement is reduction
                    improvement = ((fixed_value - algo_value) / fixed_value * 100) if fixed_value > 0 else 0
                else:
                    # Higher is better, so improvement is increase
                    improvement = ((algo_value - fixed_value) / fixed_value * 100) if fixed_value > 0 else 0
                
                improvement_data[algo][metric] = improvement
        
        # Create dataframe for plotting
        df = pd.DataFrame(improvement_data).T
        df.index = [pretty_labels.get(algo, algo) for algo in df.index]
        
        # Set the colors for each metric
        colors_map = {
            'Detection Rate': colors['detection'],
            'False Positive Rate': colors['false_positive'],
            'False Negative Rate': colors['false_negative'],
            'Accuracy': colors['accuracy']
        }
        
        # Plot horizontal bars for each algorithm and metric
        for i, algo in enumerate(df.index):
            for j, metric in enumerate(main_metrics):
                value = df.loc[algo, metric]
                color = colors_map[metric]
                # Use hatching for negative improvements
                hatch = '//' if value < 0 else None
                alpha = 0.7 if value < 0 else 1.0
                
                bar = ax.barh(i - 0.3 + j*0.2, value, height=0.15, 
                           label=metric if i == 0 else "", 
                           color=color, edgecolor='black', linewidth=1,
                           hatch=hatch, alpha=alpha)
                
                # Add value label
                text_x = value + (5 if value >= 0 else -10)
                ax.text(text_x, i - 0.3 + j*0.2, f"{value:.1f}%", 
                      va='center', ha='left' if value >= 0 else 'right',
                      fontsize=9, fontweight='bold', color='black')
        
        # Add labels and title
        ax.set_title('Improvement of Dynamic Algorithms Over Fixed Weights',
                   fontweight='bold', fontsize=16, pad=15)
        ax.set_xlabel('Percentage Improvement (%)', fontweight='bold', fontsize=14)
        ax.set_yticks(np.arange(len(df)))
        ax.set_yticklabels(df.index, fontweight='bold')
        
        # Add a reference line at 0%
        ax.axvline(x=0, color='black', linestyle='-', linewidth=1)
        
        # Set x-axis range with some padding
        max_improvement = max(df.values.max(), 100)  # At least 100% for visibility
        min_improvement = min(df.values.min(), -30)  # At least -30% for visibility
        ax.set_xlim(min_improvement - 10, max_improvement + 10)
        
        # Add grid lines
        ax.grid(axis='x', linestyle='--', alpha=0.3)
        
        # Add legend at the bottom
        handles, labels = ax.get_legend_handles_labels()
        ax.legend(handles, labels, ncol=4, bbox_to_anchor=(0.5, -0.15),
                loc='upper center', frameon=True, framealpha=0.9)
        
        # Add annotation for interpretation
        plt.figtext(0.5, 0.01, 
                  "Note: Higher percentages indicate better performance. Negative values indicate the dynamic algorithm performed worse than fixed weights.",
                  ha='center', fontsize=9, style='italic')
        
        # Identify the best performing algorithm
        best_overall = df.mean(axis=1).idxmax()
        
        # Add annotation for the best performer
        plt.figtext(0.05, 0.95, 
                  f"Best Overall: {best_overall}",
                  ha='left', fontsize=11, fontweight='bold',
                  bbox=dict(facecolor='white', alpha=0.8, boxstyle='round,pad=0.3'))
        
        # Save figure with tight layout
        plt.tight_layout(rect=[0, 0.05, 1, 0.95])
        plt.savefig(f"{self.results_dir}/improvement_analysis.pdf")
        plt.savefig(f"{self.results_dir}/improvement_analysis.png", dpi=300)
        plt.close()
    
    def _plot_tradeoff_chart(self, avg_metrics, std_metrics, pretty_labels):
        """Plot detection rate vs false positive rate tradeoff"""
        # Create figure
        fig, ax = plt.subplots(figsize=(10, 8))
        
        # Extract algorithms and set colors
        algos = list(avg_metrics.keys())
        algo_colors = {
            'fixed_weights': colors['fixed_weights'],
            'epsilon_greedy': colors['epsilon_greedy'],
            'thompson': colors['thompson'],
            'tree_ucb': colors['ucb']
        }
        
        # Add improvement zones with clear labels
        zones = [
            # (x_min, x_max, y_min, y_max, color, alpha, label, text_pos, text_color)
            (0, 0.2, 0.8, 1.0, 'green', 0.15, 'OPTIMAL', (0.1, 0.9), 'darkgreen'),
            (0.2, 1.0, 0.8, 1.0, 'lightgreen', 0.15, 'HIGH DETECTION', (0.6, 0.9), 'darkgreen'),
            (0, 0.2, 0, 0.8, 'lightblue', 0.15, 'LOW FALSE POSITIVES', (0.1, 0.4), 'navy'),
            (0.2, 1.0, 0, 0.8, 'lightgray', 0.15, 'SUBOPTIMAL', (0.6, 0.4), 'dimgray')
        ]
        
        # Plot zones
        for x_min, x_max, y_min, y_max, color, alpha, label, (text_x, text_y), text_color in zones:
            ax.fill([x_min, x_max, x_max, x_min], [y_min, y_min, y_max, y_max], 
                  color=color, alpha=alpha)
            ax.text(text_x, text_y, label, ha='center', va='center', 
                  fontsize=10, fontweight='bold', color=text_color,
                  bbox=dict(facecolor='white', alpha=0.7, boxstyle='round,pad=0.3'))
        
        # Plot reference lines
        ax.axhline(y=0.8, color='green', linestyle='--', alpha=0.5)
        ax.axvline(x=0.2, color='blue', linestyle='--', alpha=0.5)
        
        # Plot each algorithm
        markers = ['o', 's', '^', 'D']
        for i, algo in enumerate(algos):
            fp_rate = avg_metrics[algo]['False Positive Rate']
            det_rate = avg_metrics[algo]['Detection Rate']
            fn_rate = avg_metrics[algo]['False Negative Rate']
            
            # Get standard deviations
            fp_std = std_metrics[algo]['False Positive Rate']
            det_std = std_metrics[algo]['Detection Rate']
            
            # Determine marker size based on algorithm type
            size = 150 if algo == "fixed_weights" else 180
            
            # Plot the point
            ax.scatter(fp_rate, det_rate, s=size, color=algo_colors[algo], 
                     marker=markers[i], alpha=0.8, edgecolor='black', linewidth=1.5,
                     label=f"{pretty_labels[algo]} (FN: {fn_rate:.2f})")
            
            # Add error bars
            ax.errorbar(fp_rate, det_rate, xerr=fp_std, yerr=det_std,
                      fmt='none', ecolor=algo_colors[algo], alpha=0.5, capsize=4)
            
            # Add algorithm name as label
            if algo == "fixed_weights":
                offset_x, offset_y = 0.02, -0.04
            else:
                offset_x, offset_y = 0.02, 0.04
                
            ax.annotate(pretty_labels[algo], 
                      xy=(fp_rate, det_rate),
                      xytext=(fp_rate + offset_x, det_rate + offset_y),
                      fontsize=10, fontweight='bold',
                      bbox=dict(facecolor='white', alpha=0.7, boxstyle='round,pad=0.2'))
        
        # Draw arrows from fixed weights to dynamic algorithms
        fixed_fp = avg_metrics["fixed_weights"]['False Positive Rate']
        fixed_det = avg_metrics["fixed_weights"]['Detection Rate']
        
        for algo in algos:
            if algo != "fixed_weights":
                algo_fp = avg_metrics[algo]['False Positive Rate']
                algo_det = avg_metrics[algo]['Detection Rate']
                
                # Calculate arrow colors and styles based on improvement
                improvement = False
                if algo_det >= fixed_det and algo_fp <= fixed_fp:
                    # Better in both dimensions
                    color = 'green'
                    improvement = True
                elif algo_det >= fixed_det:
                    # Better detection but worse false positives
                    color = 'orange'
                elif algo_fp <= fixed_fp:
                    # Better false positives but worse detection
                    color = 'purple'
                else:
                    # Worse in both dimensions
                    color = 'red'
                
                # Draw the arrow
                style = 'solid' if improvement else 'dashed'
                ax.annotate('', xy=(algo_fp, algo_det), xytext=(fixed_fp, fixed_det),
                          arrowprops=dict(arrowstyle='->', linestyle=style,
                                        color=color, alpha=0.7, linewidth=2))
        
        # Add labels and title
        ax.set_xlabel('False Positive Rate', fontweight='bold', fontsize=14)
        ax.set_ylabel('Detection Rate', fontweight='bold', fontsize=14)
        ax.set_title('Security-Efficiency Tradeoff Analysis\n'
                    f'(Averaged over {self.num_trials} trials)',
                    fontweight='bold', fontsize=16, pad=15)
        
        # Set axis limits and grid
        ax.set_xlim(-0.05, 1.05)
        ax.set_ylim(-0.05, 1.05)
        ax.grid(True, linestyle='--', alpha=0.3)
        
        # Add legend
        ax.legend(title='Algorithms', title_fontsize=12,
                bbox_to_anchor=(0.5, -0.15), loc='upper center',
                ncol=2, frameon=True, framealpha=0.9)
        
        # Add explanation text
        plt.figtext(0.5, 0.01,
                  "Arrows show movement from Fixed Weights to each Dynamic Algorithm.\n"
                  "Green arrows indicate improvement in both dimensions, other colors show tradeoffs.",
                  ha='center', fontsize=9, style='italic')
        
        # Save figure
        plt.tight_layout(rect=[0, 0.05, 1, 0.95])
        plt.savefig(f"{self.results_dir}/tradeoff_analysis.pdf")
        plt.savefig(f"{self.results_dir}/tradeoff_analysis.png", dpi=300)
        plt.close()
        
    def _plot_security_tradeoff(self, avg_metrics, std_metrics):
        """Plot false negative vs false positive with security implications"""
        # Create figure
        fig, ax = plt.subplots(figsize=(10, 8))
        
        # Create a prettier labels for algorithms
        pretty_labels = {
            "fixed_weights": "Fixed Weights",
            "epsilon_greedy": "Epsilon-Greedy",
            "thompson": "Thompson Sampling",
            "tree_ucb": "TreeEnsembleUCB"
        }
        
        # Extract algorithms and set colors
        algos = list(avg_metrics.keys())
        algo_colors = {
            'fixed_weights': colors['fixed_weights'],
            'epsilon_greedy': colors['epsilon_greedy'],
            'thompson': colors['thompson'],
            'tree_ucb': colors['ucb']
        }
        
        # Define security zones with clear implications
        zones = [
            # (x_min, x_max, y_min, y_max, color, alpha, label, text_pos, text_color, description)
            (0, 0.1, 0, 0.1, 'green', 0.2, 'SECURE', (0.05, 0.05), 'darkgreen', 
             "Low false positives & negatives\nHigh security with good user experience"),
            
            (0.1, 0.5, 0, 0.1, 'lightgreen', 0.2, 'CAUTIOUS', (0.3, 0.05), 'darkgreen',
             "Low false negatives, moderate false positives\nGood security with occasional user friction"),
            
            (0, 0.1, 0.1, 0.5, 'orange', 0.2, 'VULNERABLE', (0.05, 0.3), 'darkorange',
             "Low false positives, moderate false negatives\nPotential security risks with good user experience"),
            
            (0.1, 0.5, 0.1, 0.5, 'red', 0.2, 'HIGH RISK', (0.3, 0.3), 'darkred',
             "High false positives & negatives\nPoor security and user experience")
        ]
        
        # Plot security zones
        for x_min, x_max, y_min, y_max, color, alpha, label, (text_x, text_y), text_color, desc in zones:
            ax.fill([x_min, x_max, x_max, x_min], [y_min, y_min, y_max, y_max], 
                  color=color, alpha=alpha)
            ax.text(text_x, text_y, label, ha='center', va='center', 
                  fontsize=12, fontweight='bold', color=text_color)
            
            # Add small description text
            lines = desc.split('\n')
            for i, line in enumerate(lines):
                ax.text(text_x, text_y - 0.02 - i*0.015, line, ha='center', va='top', 
                      fontsize=8, color=text_color, alpha=0.8)
        
        # Plot reference lines for acceptable thresholds
        ax.axhline(y=0.1, color='black', linestyle='--', alpha=0.4)
        ax.axvline(x=0.1, color='black', linestyle='--', alpha=0.4)
        
        # Plot each algorithm
        markers = ['o', 's', '^', 'D']
        for i, algo in enumerate(algos):
            fp_rate = avg_metrics[algo]['False Positive Rate']
            fn_rate = avg_metrics[algo]['False Negative Rate']
            
            # Get standard deviations
            fp_std = std_metrics[algo]['False Positive Rate']
            fn_std = std_metrics[algo]['False Negative Rate']
            
            # Determine marker size based on algorithm type
            size = 150 if algo == "fixed_weights" else 180
            
            # Plot the point
            ax.scatter(fp_rate, fn_rate, s=size, color=algo_colors[algo], 
                     marker=markers[i], alpha=0.8, edgecolor='black', linewidth=1.5,
                     label=pretty_labels.get(algo, algo))
            
            # Add error bars
            ax.errorbar(fp_rate, fn_rate, xerr=fp_std, yerr=fn_std,
                      fmt='none', ecolor=algo_colors[algo], alpha=0.5, capsize=4)
            
            # Add algorithm name as label
            ax.annotate(pretty_labels.get(algo, algo), 
                      xy=(fp_rate, fn_rate),
                      xytext=(fp_rate + 0.02, fn_rate + 0.02),
                      fontsize=10, fontweight='bold',
                      bbox=dict(facecolor='white', alpha=0.7, boxstyle='round,pad=0.2'))
        
        # Draw arrows from fixed weights to dynamic algorithms
        fixed_fp = avg_metrics["fixed_weights"]['False Positive Rate']
        fixed_fn = avg_metrics["fixed_weights"]['False Negative Rate']
        
        for algo in algos:
            if algo != "fixed_weights":
                algo_fp = avg_metrics[algo]['False Positive Rate']
                algo_fn = avg_metrics[algo]['False Negative Rate']
                
                # Calculate arrow colors and styles based on improvement
                improvement = False
                if algo_fn <= fixed_fn and algo_fp <= fixed_fp:
                    # Better in both dimensions
                    color = 'green'
                    improvement = True
                elif algo_fn <= fixed_fn:
                    # Better false negatives (more secure) but worse false positives
                    color = 'orange'
                elif algo_fp <= fixed_fp:
                    # Better false positives but worse false negatives (less secure)
                    color = 'purple'
                else:
                    # Worse in both dimensions
                    color = 'red'
                
                # Draw the arrow
                style = 'solid' if improvement else 'dashed'
                ax.annotate('', xy=(algo_fp, algo_fn), xytext=(fixed_fp, fixed_fn),
                          arrowprops=dict(arrowstyle='->', linestyle=style,
                                        color=color, alpha=0.7, linewidth=2))
        
        # Add labels and title
        ax.set_xlabel('False Positive Rate (User Experience Impact)', fontweight='bold', fontsize=14)
        ax.set_ylabel('False Negative Rate (Security Impact)', fontweight='bold', fontsize=14)
        ax.set_title('Security Impact Analysis\n'
                    f'(Averaged over {self.num_trials} trials)',
                    fontweight='bold', fontsize=16, pad=15)
        
        # Set axis limits and grid
        ax.set_xlim(-0.05, 0.55)
        ax.set_ylim(-0.05, 0.55)
        ax.grid(True, linestyle='--', alpha=0.3)
        
        # Add legend
        ax.legend(title='Algorithms', title_fontsize=12,
                bbox_to_anchor=(0.5, -0.15), loc='upper center',
                ncol=len(algos), frameon=True, framealpha=0.9)
        
        # Add explanation text
        plt.figtext(0.5, 0.01,
                  "Lower values are better for both metrics.\n"
                  "False Negatives (missed attacks) have greater security impact than False Positives (false alarms).",
                  ha='center', fontsize=9, style='italic')
        
        # Save figure
        plt.tight_layout(rect=[0, 0.05, 1, 0.95])
        plt.savefig(f"{self.results_dir}/security_tradeoff.pdf")
        plt.savefig(f"{self.results_dir}/security_tradeoff.png", dpi=300)
        plt.close()
        
    def _plot_roc_curves(self, metrics_by_algo):
        """Plot publication-quality ROC curves for algorithm comparison"""
        # Create figure
        fig, ax = plt.subplots(figsize=(10, 8))
        
        # Define a color-blind friendly palette
        algo_colors = {
            'fixed_weights': colors['fixed_weights'],
            'epsilon_greedy': colors['epsilon_greedy'],
            'thompson': colors['thompson'],
            'tree_ucb': colors['ucb']
        }
        
        # Generate points for ROC curves by varying thresholds
        for algo_name, metrics in metrics_by_algo.items():
            # Extract risk scores for this algorithm
            # Fix: convert metrics lists to numpy arrays for proper filtering
            is_attack_array = np.array(self.metrics["is_attack"])
            algorithm_array = np.array(self.metrics["algorithm"])
            risk_score_array = np.array(self.metrics["risk_score"])
            
            # Find indices where algorithm matches
            algo_indices = np.where(algorithm_array == algo_name)[0]
            
            # Extract data for this algorithm
            algo_is_attack = is_attack_array[algo_indices]
            algo_risk_score = risk_score_array[algo_indices]
            
            # Generate ROC curve points
            fpr = []  # False positive rates
            tpr = []  # True positive rates
            
            # Calculate points along ROC curve
            thresholds = np.linspace(0, 1, 20)
            for threshold in thresholds:
                tp = np.sum((algo_is_attack == True) & (algo_risk_score > threshold))
                fp = np.sum((algo_is_attack == False) & (algo_risk_score > threshold))
                fn = np.sum((algo_is_attack == True) & (algo_risk_score <= threshold))
                tn = np.sum((algo_is_attack == False) & (algo_risk_score <= threshold))
                
                # Calculate rates
                fpr.append(fp / (fp + tn) if (fp + tn) > 0 else 0)
                tpr.append(tp / (tp + fn) if (tp + fn) > 0 else 0)
            
            # Calculate AUC
            auc = np.trapz(tpr, fpr) if len(fpr) > 1 else 0
            
            # Plot ROC curve with clear styling
            ax.plot(fpr, tpr, 
                   color=algo_colors[algo_name], 
                   marker='o', 
                   markersize=5,
                   linewidth=2, 
                   label=f"{algo_name} (AUC = {auc:.3f})")
            
            # Mark operating point (actual threshold used)
            actual_fpr = metrics['False Positive Rate']
            actual_tpr = metrics['Detection Rate']
            ax.scatter([actual_fpr], [actual_tpr], 
                      s=150, 
                      marker='X', 
                      edgecolor='black',
                      linewidth=1.5,
                      color=algo_colors[algo_name])
        
        # Add diagonal reference line
        ax.plot([0, 1], [0, 1], color='gray', linestyle='--', alpha=0.7, linewidth=1.5)
        
        # Add labels and title
        ax.set_xlabel('False Positive Rate', fontweight='bold', fontsize=14)
        ax.set_ylabel('True Positive Rate (Detection Rate)', fontweight='bold', fontsize=14)
        ax.set_title('ROC Curves for Attack Detection Algorithms', 
                   fontweight='bold', fontsize=16, pad=15)
        
        # Set axis limits with slight padding
        ax.set_xlim([-0.02, 1.02])
        ax.set_ylim([-0.02, 1.02])
        
        # Add grid with subtle styling
        ax.grid(True, linestyle='--', alpha=0.3)
        
        # Add legend with clean styling
        ax.legend(loc='lower right', frameon=True, framealpha=0.9, edgecolor='gray')
        
        # Add explanation of ROC curves
        plt.figtext(0.5, 0.01,
                  "Note: Higher AUC indicates better performance. Curves closer to top-left corner show better detection with fewer false positives.",
                  ha='center', fontsize=10, style='italic')
        
        # Save figure with tight layout
        plt.tight_layout(rect=[0, 0.03, 1, 0.97])
        plt.savefig(f"{self.results_dir}/roc_curves.pdf")
        plt.savefig(f"{self.results_dir}/roc_curves.png", dpi=300)
        plt.close()

    def _plot_performance_over_time(self):
        """Plot performance metrics over time to show adaptation to context drift"""
        # Convert metrics to DataFrame
        df = pd.DataFrame(self.metrics)
        
        # Create figure with multiple subplots
        fig, axes = plt.subplots(2, 1, figsize=(12, 10), sharex=True)
        
        # Plot detection rate and false positive rate over time
        window_size = max(50, len(df) // 100)  # Dynamic window size
        
        for algo_name in self.bandit_algorithms.keys():
            algo_df = df[df["algorithm"] == algo_name]
            
            # Calculate moving average of detection rate and false positive rate
            detection_rates = []
            false_pos_rates = []
            timestamps = []
            
            for i in range(0, len(algo_df), window_size//2):
                window = algo_df.iloc[i:i+window_size]
                if len(window) < window_size//2:
                    continue
                    
                # Calculate rates for this window
                attacks = sum(window["is_attack"])
                if attacks > 0:
                    detected = sum((window["is_attack"]) & (window["attack_detected"]))
                    detection_rate = detected / attacks
                    detection_rates.append(detection_rate)
                    
                    # False positive rate
                    normals = sum(~window["is_attack"])
                    if normals > 0:
                        false_pos = sum((~window["is_attack"]) & (window["attack_detected"]))
                        false_pos_rate = false_pos / normals
                        false_pos_rates.append(false_pos_rate)
                    else:
                        false_pos_rates.append(0)
                        
                    timestamps.append(window["timestamp"].mean())
            
            # Convert timestamps to hours from start
            if timestamps:
                start_time = df["timestamp"].min()
                hours = [(t - start_time).total_seconds() / 3600 for t in timestamps]
                
                # Plot detection rate
                axes[0].plot(hours, detection_rates, 
                           label=algo_name, 
                           color=colors[algo_name], 
                           linewidth=2.5)
                          
                # Plot false positive rate
                axes[1].plot(hours, false_pos_rates, 
                           color=colors[algo_name],
                           linewidth=2.5)
        
        # Add drift points as vertical lines
        if self.enable_drift:
            for drift_time in self.drift_points:
                drift_hour = (drift_time - df["timestamp"].min()).total_seconds() / 3600
                for ax in axes:
                    ax.axvline(x=drift_hour, color='red', alpha=0.4, linestyle='-', linewidth=2)
                    ax.text(drift_hour + 0.5, 0.05, "Context\nDrift", 
                          rotation=90, color='red', alpha=0.7, fontsize=10, fontweight='bold',
                          bbox=dict(facecolor='white', alpha=0.7, boxstyle='round,pad=0.3'))
        
        # Configure axes
        axes[0].set_ylabel("Detection Rate", fontweight='bold', fontsize=14)
        axes[0].set_ylim(0, 1.05)
        axes[0].grid(True, linestyle='--', alpha=0.3)
        axes[0].legend(loc='lower right', frameon=True, framealpha=0.9)
        axes[0].set_title("Algorithm Performance Over Time", fontweight='bold', fontsize=16, pad=15)
        
        axes[1].set_xlabel("Hours", fontweight='bold', fontsize=14)
        axes[1].set_ylabel("False Positive Rate", fontweight='bold', fontsize=14)
        axes[1].set_ylim(0, 0.8)
        axes[1].grid(True, linestyle='--', alpha=0.3)
        
        # Add explanation text
        plt.figtext(0.5, 0.01,
                  "Note: Dynamic algorithms adapt after context drift, while fixed weights algorithm shows degraded performance.",
                  ha='center', fontsize=10, style='italic')
        
        # Save figure
        plt.tight_layout(rect=[0, 0.03, 1, 0.97])
        plt.savefig(f"{self.results_dir}/performance_over_time.pdf")
        plt.savefig(f"{self.results_dir}/performance_over_time.png", dpi=300)
        plt.close()

    def _plot_weight_evolution(self):
        """Plot how weights evolve over time for dynamic algorithms"""
        # Only run if we're tracking weights over time
        if not hasattr(self, 'weight_history'):
            print("Weight history not found. Skipping weight evolution plot.")
            return
        
        # Verify we have data to plot
        if not self.weight_history["timestamp"]:
            print("No weight history data to plot.")
            return
            
        # Create figure with subplots for each algorithm
        fig, axes = plt.subplots(3, 1, figsize=(12, 15), sharex=True)
        
        # Get unique timestamps and convert to hours
        start_time = min(self.weight_history["timestamp"])
        hours = [(t - start_time).total_seconds() / 3600 for t in self.weight_history["timestamp"]]
        
        # Plot for each dynamic algorithm
        algo_names = ["epsilon_greedy", "thompson", "tree_ucb"]
        
        # Use a consistent color map for factors across all algorithms
        factor_colors = {
            'time': '#66C2A5',           # Teal
            'failed_attempts': '#FC8D62', # Orange
            'user_behavior': '#8DA0CB',   # Blue
            'motion': '#E78AC3',         # Pink
            'network': '#A6D854'          # Green
        }
        
        # Track whether any algorithm has data to plot
        has_data = False
        
        for i, algo in enumerate(algo_names):
            ax = axes[i]
            
            # Skip if algorithm isn't in the history
            if algo not in self.weight_history:
                continue
                
            # Check if this algorithm has any weight data
            algo_has_data = False
                
            # Plot each factor's weight over time
            for factor in self.risk_factors:
                if factor not in self.weight_history[algo]:
                    continue
                    
                weights = self.weight_history[algo][factor]
                
                # Skip empty weights arrays
                if not weights or len(weights) == 0:
                    continue
                
                # Filter out None values for plotting
                valid_indices = [i for i, w in enumerate(weights) if w is not None]
                if not valid_indices:
                    continue
                    
                valid_hours = [hours[i] for i in valid_indices]
                valid_weights = [weights[i] for i in valid_indices]
                
                # Plot with clean data
                ax.plot(valid_hours, valid_weights, 
                      label=factor,
                      color=factor_colors[factor],
                      linewidth=2.5)
                
                # Mark that we have data
                algo_has_data = True
                has_data = True
            
            # Add context change markers - these represent changes in context key
            # This helps visualize how bandit weights change based on context
            if algo_has_data and len(self.weight_history["context_keys"]) > 0:
                prev_key = None
                for j, key in enumerate(self.weight_history["context_keys"]):
                    if j > 0 and key != prev_key:
                        # Context change (e.g., hour bin changed)
                        ax.axvline(x=hours[j], color='black', alpha=0.1, linestyle='-')
                    prev_key = key
            
            # Add drift points as vertical lines
            if algo_has_data and self.enable_drift:
                for drift_time in self.drift_points:
                    drift_hour = (drift_time - start_time).total_seconds() / 3600
                    ax.axvline(x=drift_hour, color='red', alpha=0.3, linestyle='-')
                    ax.text(drift_hour + 0.5, ax.get_ylim()[0] + 0.1, "Drift", 
                          rotation=90, color='red', alpha=0.7, fontsize=10, fontweight='bold')
            
            # Configure axis
            ax.set_title(f"{algo.replace('_', ' ').title()} Weight Evolution", fontweight='bold', fontsize=14)
            
            # Determine y-axis limits dynamically
            if algo_has_data:
                # Find max and min weights for this algorithm across all factors
                factor_weights = [self.weight_history[algo][f] for f in self.risk_factors 
                                 if f in self.weight_history[algo] and len(self.weight_history[algo][f]) > 0]
                
                if factor_weights:
                    max_weight = max([max(weights) for weights in factor_weights])
                    min_weight = min([min(weights) for weights in factor_weights])
                    margin = (max_weight - min_weight) * 0.1
                    ax.set_ylim(max(0, min_weight - margin), max(3.0, max_weight + margin))
                else:
                    ax.set_ylim(0, 3.0)
            else:
                ax.set_ylim(0, 3.0)
                
            ax.set_ylabel("Weight Value", fontweight='bold', fontsize=14)
            ax.grid(True, linestyle='--', alpha=0.3)
            
            # Only add legend if there's data
            if algo_has_data:
                ax.legend(loc='upper right', frameon=True, framealpha=0.9)
            else:
                ax.text(0.5, 0.5, "No weight data available", 
                      horizontalalignment='center',
                      verticalalignment='center',
                      transform=ax.transAxes,
                      fontsize=12,
                      style='italic',
                      color='gray')
        
        # X-axis label on bottom plot only
        axes[2].set_xlabel("Hours", fontweight='bold', fontsize=14)
        
        # Format x-axis with day markers if simulation runs for multiple days
        if hours and max(hours) > 24:
            day_ticks = np.arange(0, max(hours) + 24, 24)
            day_labels = [f"Day {int(h/24)+1}" for h in day_ticks]
            axes[2].set_xticks(day_ticks)
            axes[2].set_xticklabels(day_labels)
        
        # Add overall title - FIXED: removed 'pad' parameter
        fig.suptitle("Risk Factor Weight Evolution Over Time", fontweight='bold', fontsize=16)
        
        # Add explanation text
        plt.figtext(0.5, 0.01,
                  "Note: Weights adapt in response to context drift events and context changes (faint gray lines).\n"
                  "Contextual bandits learn different weights for different time periods (morning, afternoon, etc).",
                  ha='center', fontsize=10, style='italic')
        
        # If no data at all, add a clear message
        if not has_data:
            fig.text(0.5, 0.5, "No weight data available for any algorithm",
                   ha='center', va='center', fontsize=16, color='gray')
        
        # Save figure
        plt.tight_layout(rect=[0, 0.03, 1, 0.97])
        plt.savefig(f"{self.results_dir}/weight_evolution.pdf")
        plt.savefig(f"{self.results_dir}/weight_evolution.png", dpi=300)
        plt.close()

    def _plot_contextual_weights(self):
        """Plot heatmaps showing how weights vary across contexts (day/hour)"""
        # Skip if not enough data
        if not hasattr(self, 'weight_history') or len(self.weight_history["timestamp"]) < 10:
            print("Insufficient weight history for contextual weight plot")
            return
        
        # Create figure with subplots - one row per algorithm, one column per factor
        algo_names = ["epsilon_greedy", "thompson", "tree_ucb"]
        factor_names = list(self.risk_factors.keys())
        
        fig, axes = plt.subplots(len(algo_names), len(factor_names), 
                                figsize=(15, 10), 
                                sharex=True, sharey=True)
        
        # Create bins for days and hours
        days = range(7)  # 0-6 (Monday-Sunday)
        hours = range(0, 24, 4)  # 0, 4, 8, 12, 16, 20 (4-hour bins)
        
        # Create a matrix to store average weights for each context
        context_weights = {}
        for algo in algo_names:
            context_weights[algo] = {}
            for factor in factor_names:
                # Initialize weight matrix with NaN (7 days x 6 hour bins)
                context_weights[algo][factor] = np.full((7, 6), np.nan)
        
        # Collect weights by context
        context_counts = np.zeros((7, 6))
        for i, timestamp in enumerate(self.weight_history["timestamp"]):
            day = timestamp.weekday()
            hour_bin = timestamp.hour // 4
            
            context_counts[day, hour_bin] += 1
            
            for algo in algo_names:
                if algo not in self.weight_history:
                    continue
                
                for factor in factor_names:
                    if factor not in self.weight_history[algo]:
                        continue
                    
                    # Make sure we have data at this index
                    if i >= len(self.weight_history[algo][factor]):
                        continue
                        
                    # Skip None values
                    if self.weight_history[algo][factor][i] is None:
                        continue
                        
                    current_val = context_weights[algo][factor][day, hour_bin]
                    # If NaN, initialize, otherwise update running average
                    if np.isnan(current_val):
                        context_weights[algo][factor][day, hour_bin] = self.weight_history[algo][factor][i]
                    else:
                        # Weighted average with new sample
                        context_weights[algo][factor][day, hour_bin] = (
                            current_val * (context_counts[day, hour_bin] - 1) + 
                            self.weight_history[algo][factor][i]
                        ) / context_counts[day, hour_bin]
        
        # Create a pretty colormap with good contrast
        cmap = plt.cm.viridis
        
        # Common min/max for better comparison across algorithms
        vmin = np.nanmin([np.nanmin(context_weights[algo][factor]) 
                        for algo in algo_names 
                        for factor in factor_names])
        vmax = np.nanmax([np.nanmax(context_weights[algo][factor]) 
                        for algo in algo_names 
                        for factor in factor_names])
        
        # Round to nearest 0.5 and add margin
        vmin = max(0, np.floor(vmin * 2) / 2)
        vmax = min(3, np.ceil(vmax * 2) / 2 + 0.5)
        
        # Day and hour labels
        day_labels = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']
        hour_labels = ['0-3', '4-7', '8-11', '12-15', '16-19', '20-23']
        
        # Plot each heatmap
        for i, algo in enumerate(algo_names):
            for j, factor in enumerate(factor_names):
                ax = axes[i, j]
                
                # Create heatmap - transpose to make days as rows, hours as columns
                im = ax.imshow(context_weights[algo][factor], 
                              cmap=cmap, aspect='auto', 
                              vmin=vmin, vmax=vmax)
                
                # Configure axis
                if i == 0:
                    ax.set_title(factor.replace('_', ' ').title(), 
                                fontweight='bold', fontsize=12)
                
                if j == 0:
                    ax.set_ylabel(algo.replace('_', ' ').title(),
                                 fontweight='bold', fontsize=12)
                
                # Set tick labels
                if i == len(algo_names) - 1:
                    ax.set_xticks(np.arange(len(hour_labels)))
                    ax.set_xticklabels(hour_labels, rotation=45)
                    ax.set_xlabel('Hour of Day', fontsize=10)
                else:
                    ax.set_xticks([])
                
                if j == 0:
                    ax.set_yticks(np.arange(len(day_labels)))
                    ax.set_yticklabels(day_labels)
                else:
                    ax.set_yticks([])
                
                # Add values as text in each cell
                for day_idx in range(len(day_labels)):
                    for hour_idx in range(len(hour_labels)):
                        value = context_weights[algo][factor][day_idx, hour_idx]
                        if not np.isnan(value):
                            # Choose text color based on background brightness
                            color = 'white' if value > (vmin + vmax) / 2 else 'black'
                            ax.text(hour_idx, day_idx, f"{value:.2f}", 
                                   ha="center", va="center", 
                                   color=color, fontsize=8)
        
        # Add colorbar
        cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])
        cbar = fig.colorbar(im, cax=cbar_ax)
        cbar.set_label('Weight Value', fontweight='bold')
        
        # Add overall title
        fig.suptitle('Contextual Weights by Day and Hour', 
                    fontweight='bold', fontsize=16, y=0.98)
        
        # Add explanation
        plt.figtext(0.5, 0.02,
                  "Note: This visualization shows how weights vary across different contexts (day and time).\n"
                  "Contextual bandits learn to assign different importance to risk factors based on context patterns.",
                  ha='center', fontsize=10, style='italic')
        
        # Save figure
        plt.tight_layout(rect=[0, 0.05, 0.9, 0.95])
        plt.savefig(f"{self.results_dir}/contextual_weights.pdf")
        plt.savefig(f"{self.results_dir}/contextual_weights.png", dpi=300)
        plt.close()

class MultiSimulationRunner:
    """Runs multiple simulations and averages the results"""
    
    def __init__(self, num_trials=5, duration_hours=24, time_step_minutes=15, seed=None):
        self.num_trials = num_trials
        self.duration_hours = duration_hours
        self.time_step_minutes = time_step_minutes
        self.results_dir = "results"
        os.makedirs(self.results_dir, exist_ok=True)
        
        # If seed is provided, use it to initialize the random number generator
        if seed is not None:
            random.seed(seed)
            np.random.seed(seed)
        
        # Storage for metrics across all trials
        self.all_metrics = {
            "overall": defaultdict(list),
            "pre_drift": defaultdict(list),
            "post_drift": defaultdict(list)
        }
        
        # Track performance across trials for statistical analysis
        self.trial_metrics = []
        
    def run_simulations(self):
        """Run multiple simulations and collect results"""
        print(f"Running {self.num_trials} simulation trials...")
        
        for trial in range(1, self.num_trials + 1):
            print(f"\nTrial {trial}/{self.num_trials}")
            # Create and run a new simulation
            sim = RiskAssessmentSimulation(
                duration_hours=self.duration_hours, 
                time_step_minutes=self.time_step_minutes
            )
            sim.run_simulation()
            
            # Get metrics but don't plot individual trial results
            metrics, pre_metrics, post_metrics = sim.analyze_results()
            
            # Save metrics from this trial
            self.trial_metrics.append({
                "overall": metrics,
                "pre_drift": pre_metrics,
                "post_drift": post_metrics
            })
            
            # Accumulate metrics
            for algo, algo_metrics in metrics.items():
                for metric_name, metric_value in algo_metrics.items():
                    self.all_metrics["overall"][f"{algo}_{metric_name}"].append(metric_value)
            
            if pre_metrics:
                for algo, algo_metrics in pre_metrics.items():
                    for metric_name, metric_value in algo_metrics.items():
                        self.all_metrics["pre_drift"][f"{algo}_{metric_name}"].append(metric_value)
            
            if post_metrics:
                for algo, algo_metrics in post_metrics.items():
                    for metric_name, metric_value in algo_metrics.items():
                        self.all_metrics["post_drift"][f"{algo}_{metric_name}"].append(metric_value)
        
        # Calculate and display averaged metrics
        self.analyze_averaged_results()
    
    def analyze_averaged_results(self):
        """Calculate and display averaged metrics"""
        print("\n" + "="*80)
        print(f"AVERAGED RESULTS ACROSS {self.num_trials} TRIALS")
        print("="*80)
        
        # Create reconstructed metrics for plotting
        avg_metrics = {}
        std_metrics = {}
        main_metrics = ["Detection Rate", "False Positive Rate", "False Negative Rate", "Accuracy"]
        
        # Calculate averages and std devs for each algorithm and metric
        for algo in ["fixed_weights", "epsilon_greedy", "thompson", "tree_ucb"]:
            avg_metrics[algo] = {}
            std_metrics[algo] = {}
            
            for metric in main_metrics:
                values = self.all_metrics["overall"][f"{algo}_{metric}"]
                if values:
                    avg_metrics[algo][metric] = np.mean(values)
                    std_metrics[algo][metric] = np.std(values)
                else:
                    avg_metrics[algo][metric] = 0
                    std_metrics[algo][metric] = 0
                    
            # Add attack and normal counts
            for count_metric in ["Attack Count", "Normal Count"]:
                values = self.all_metrics["overall"][f"{algo}_{count_metric}"]
                if values:
                    avg_metrics[algo][count_metric] = np.mean(values)
                    std_metrics[algo][count_metric] = np.std(values)
                else:
                    avg_metrics[algo][count_metric] = 0
                    std_metrics[algo][count_metric] = 0
        
        # Display averaged results
        print("\nAVERAGED PERFORMANCE METRICS BY ALGORITHM:")
        print("-" * 120)
        print(f"{'Algorithm':<15} {'Detection Rate':<21} {'False Pos Rate':<21} {'False Neg Rate':<21} {'Accuracy':<21}")
        print(f"{'':15} {'Mean':10} {'StdDev':10} {'Mean':10} {'StdDev':10} {'Mean':10} {'StdDev':10}")
        print("-" * 120)
        
        for algo in avg_metrics:
            print(f"{algo:<15} "
                  f"{avg_metrics[algo]['Detection Rate']:10.3f} {std_metrics[algo]['Detection Rate']:10.3f} "
                  f"{avg_metrics[algo]['False Positive Rate']:10.3f} {std_metrics[algo]['False Positive Rate']:10.3f} "
                  f"{avg_metrics[algo]['False Negative Rate']:10.3f} {std_metrics[algo]['False Negative Rate']:10.3f} "
                  f"{avg_metrics[algo]['Accuracy']:10.3f} {std_metrics[algo]['Accuracy']:10.3f}")
        
        # Calculate dynamic average
        dynamic_algos = ["epsilon_greedy", "thompson", "tree_ucb"]
        dynamic_avg = {
            metric: np.mean([avg_metrics[algo][metric] for algo in dynamic_algos])
            for metric in main_metrics
        }
        
        # Display fixed vs dynamic comparison
        print("\nAVERAGED FIXED VS DYNAMIC COMPARISON:")
        print("-" * 80)
        print(f"{'Metric':<20} {'Fixed Weights':<15} {'Dynamic Avg':<15} {'Improvement':<15}")
        print("-" * 80)
        
        for metric in main_metrics:
            fixed_value = avg_metrics["fixed_weights"][metric]
            dynamic_value = dynamic_avg[metric]
            
            if metric in ["False Positive Rate", "False Negative Rate"]:
                # Lower is better
                improvement = ((fixed_value - dynamic_value) / fixed_value * 100) if fixed_value > 0 else float('inf')
                improvement_str = f"+{improvement:.1f}%" if improvement > 0 else f"{improvement:.1f}%"
            else:
                # Higher is better
                improvement = ((dynamic_value - fixed_value) / fixed_value * 100) if fixed_value > 0 else float('inf')
                improvement_str = f"+{improvement:.1f}%" if improvement > 0 else f"{improvement:.1f}%"
                
            print(f"{metric:<20} {fixed_value:<15.3f} {dynamic_value:<15.3f} {improvement_str:<15}")
        
        # Create summary CSV files
        self._create_summary_tables(avg_metrics, std_metrics, main_metrics)
        
        # Generate plots for averaged results
        sim = RiskAssessmentSimulation()
        sim.num_trials = self.num_trials  # Pass the number of trials to the simulation
        sim._plot_averaged_results(avg_metrics, std_metrics)
        
        print(f"\nSummary results saved to {self.results_dir}/")
    
    def _create_summary_tables(self, avg_metrics, std_metrics, main_metrics):
        """Create summary tables of results"""
        # Create directory
        results_dir = Path(self.results_dir)
        results_dir.mkdir(exist_ok=True)
        
        # Create DataFrame for average metrics
        avg_df = pd.DataFrame()
        std_df = pd.DataFrame()
        
        # Fill DataFrames
        for algo in avg_metrics:
            for metric in main_metrics:
                avg_df.loc[algo, metric] = avg_metrics[algo][metric]
                std_df.loc[algo, metric] = std_metrics[algo][metric]
        
        # Add Security Score
        for algo in avg_metrics:
            dr = avg_metrics[algo]["Detection Rate"]
            fpr = avg_metrics[algo]["False Positive Rate"]
            fnr = avg_metrics[algo]["False Negative Rate"]
            security_score = dr - (2 * fnr) - (0.5 * fpr)
            security_score = max(0, min(1, security_score))
            avg_df.loc[algo, "Security Score"] = security_score
        
        # Calculate improvement over fixed weights
        improvement_df = pd.DataFrame()
        for algo in avg_df.index:
            if algo != "fixed_weights":
                for metric in avg_df.columns:
                    fixed_value = avg_df.loc["fixed_weights", metric]
                    algo_value = avg_df.loc[algo, metric]
                    
                    if metric in ["False Positive Rate", "False Negative Rate"]:
                        # Lower is better
                        improvement = ((fixed_value - algo_value) / fixed_value * 100) if fixed_value > 0 else float('inf')
                    else:
                        # Higher is better
                        improvement = ((algo_value - fixed_value) / fixed_value * 100) if fixed_value > 0 else float('inf')
                        
                    improvement_df.loc[algo, metric] = improvement
        
        # Format DataFrames
        avg_df = avg_df.round(3)
        std_df = std_df.round(3)
        improvement_df = improvement_df.round(1)
        
        # Save to CSV
        avg_df.to_csv(f"{results_dir}/average_metrics.csv")
        std_df.to_csv(f"{results_dir}/std_metrics.csv")
        improvement_df.to_csv(f"{results_dir}/improvement_metrics.csv")
        
        # Create a combined summary table with mean  std
        summary_df = pd.DataFrame()
        for algo in avg_df.index:
            for metric in avg_df.columns:
                mean = avg_df.loc[algo, metric]
                std = std_df.loc[algo, metric] if metric in std_df.columns else np.nan
                summary_df.loc[algo, metric] = f"{mean:.3f}  {std:.3f}" if not np.isnan(std) else f"{mean:.3f}"
                
        # Save to CSV
        summary_df.to_csv(f"{results_dir}/summary_metrics.csv")
        


if __name__ == "__main__":
    try:
        # Parse command line arguments
        parser = argparse.ArgumentParser(description='Run risk assessment simulations')
        parser.add_argument('--trials', type=int, default=5,
                           help='Number of simulation trials to run (default: 5)')
        parser.add_argument('--hours', type=int, default=24,
                           help='Duration of each simulation in hours (default: 24)')
        parser.add_argument('--minutes', type=int, default=15,
                           help='Time step in minutes (default: 15)')
        parser.add_argument('--seed', type=int, default=42,
                           help='Random seed for reproducibility (default: None)')
        parser.add_argument('--single', action='store_true',
                           help='Run a single simulation instead of multiple trials')
        
        args = parser.parse_args()
        
        if args.single:
            # Run a single simulation
            print("Running a single simulation...")
            if args.seed is not None:
                random.seed(args.seed)
                np.random.seed(args.seed)
                
            sim = RiskAssessmentSimulation(
                duration_hours=args.hours, 
                time_step_minutes=args.minutes
            )
            sim.run_simulation()
            sim.analyze_results()
            sim._plot_results(sim._calculate_metrics(pd.DataFrame(sim.metrics)))
        else:
            # Run multiple simulations and average results
            runner = MultiSimulationRunner(
                num_trials=args.trials,
                duration_hours=args.hours,
                time_step_minutes=args.minutes,
                seed=args.seed
            )
            runner.run_simulations()
            
    except Exception as e:
        import traceback
        print(f"Error: {e}")
        traceback.print_exc() 